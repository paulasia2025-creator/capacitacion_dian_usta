{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d255c28-5ee0-4267-a41f-8c036039c114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Taller de Arquitecturas de Datos\n",
    "**🔬 Paso 1: Creando Nuestro Universo de Datos**\n",
    "\n",
    "¡Bienvenidos al taller práctico! Antes de poder construir y comparar arquitecturas, necesitamos la materia prima: los datos.\n",
    "\n",
    "En esta celda, ejecutaremos un script de PySpark que simula un ecosistema de datos completo para una empresa de E-commerce. Este no es un dataset estático; es un generador que creará un universo de datos controlado y realista para nuestro laboratorio.\n",
    "\n",
    "**¿Qué Genera este Script?**\n",
    "Este código creará varios \"activos de datos\" que simulan las diferentes fuentes que encontrarías en un entorno real:\n",
    "\n",
    "* **Tablas Estructuradas:** usuarios, productos y pedidos, simulando los datos de una base de datos transaccional.\n",
    "* **Logs Semi-Estructurados:** logs_web con la actividad de navegación de los usuarios.\n",
    "* **Archivos No Estructurados:** Facturas en formato de texto guardadas en DBFS, simulando la ingesta de archivos como PDFs.\n",
    "* **Documentos Complejos:** Un perfil_360 de cliente, simulando cómo se verían los datos en una base de datos NoSQL como Cosmos DB o MongoDB.\n",
    "\n",
    "**Puntos Clave:**\n",
    "* **Librería Faker:** Usamos esta librería para generar datos que parecen reales (nombres, emails, fechas, etc.).\n",
    "* **Semilla de Reproducibilidad:** Hemos fijado una SEMILLA para que cada vez que se ejecute el script, genere exactamente los mismos datos. Elemental para que todos obtengamos los mismos resultados en los ejercicios.\n",
    "\n",
    "**Acción:** Ejecuta esta celda para generar todos los DataFrames y archivos necesarios. ¡Este es el punto de partida para nuestro viaje a través de las arquitecturas de datos!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3a4adb-d601-4d98-91d5-68dd1894eaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Script para generar datos simulados para el Taller de Arquitecturas de Datos\n",
    "# Este script utiliza PySpark y la librería Faker para crear datos realistas,\n",
    "# incluyendo la simulación de archivos de texto (facturas) y logs web.\n",
    "#\n",
    "# Instrucciones en Databricks:\n",
    "# 1. Asegúrate de que la librería 'Faker' esté instalada en tu cluster.\n",
    "#    Puedes hacerlo a través de la UI del cluster en la pestaña \"Libraries\".\n",
    "#    - PyPI -> package: Faker\n",
    "# 2. Copia y pega este código en una celda de un notebook de Databricks.\n",
    "# 3. Ejecuta la celda.\n",
    "\n",
    "%pip install Faker\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, ArrayType, MapType\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Inicializar Faker para generar datos falsos\n",
    "fake = Faker('es_ES') # Usar localización en español para datos más realistas\n",
    "\n",
    "# --- SEMILLA PARA REPRODUCIBILIDAD ---\n",
    "# Establecemos una semilla para que los datos generados sean siempre los mismos en cada ejecución.\n",
    "# Esto es crucial para que todos los estudiantes trabajen con el mismo dataset.\n",
    "SEED = 42\n",
    "Faker.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- 1. Inicialización de Spark Session ---\n",
    "# En un notebook de Databricks, la sesión de Spark ya está creada como 'spark'.\n",
    "print(\"Spark Session iniciada.\")\n",
    "\n",
    "# --- 2. Funciones de Generación de Datos Relacionales ---\n",
    "\n",
    "def generar_usuarios(n=100):\n",
    "    \"\"\"Genera una lista de diccionarios de usuarios.\"\"\"\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_usuario': 1000 + i,\n",
    "            'nombre': fake.name(),\n",
    "            'email': fake.email(),\n",
    "            'fecha_registro': fake.date_time_between(start_date='-2y', end_date='now'),\n",
    "            'ciudad': fake.city()\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_productos(n=50):\n",
    "    \"\"\"Genera una lista de diccionarios de productos.\"\"\"\n",
    "    categorias = ['Electrónica', 'Hogar', 'Ropa', 'Libros', 'Deportes']\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_producto': 2000 + i,\n",
    "            'nombre_producto': fake.word().capitalize() + \" \" + fake.word(),\n",
    "            'categoria': random.choice(categorias),\n",
    "            'precio_unitario': round(random.uniform(5.0, 250.0), 2)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_pedidos(usuarios, productos, n=500):\n",
    "    \"\"\"Genera una lista de diccionarios de pedidos, vinculando usuarios y productos.\"\"\"\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        producto = random.choice(productos)\n",
    "        cantidad = random.randint(1, 5)\n",
    "        data.append({\n",
    "            'id_pedido': 3000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'id_producto': producto['id_producto'],\n",
    "            'cantidad': cantidad,\n",
    "            'monto': round(cantidad * producto['precio_unitario'], 2),\n",
    "            'fecha_pedido': fake.date_time_between(start_date=usuario['fecha_registro'], end_date='now')\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def generar_logs_web(usuarios, n=2000):\n",
    "    \"\"\"Genera una lista de diccionarios de logs de visitas web más realistas.\"\"\"\n",
    "    paginas = ['/inicio', '/producto/detalle', '/carrito', '/checkout', '/perfil']\n",
    "    metodos = ['GET', 'GET', 'GET', 'POST', 'GET']\n",
    "    status = [200, 200, 200, 200, 404, 500]\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        data.append({\n",
    "            'id_log': 4000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'pagina_visitada': random.choice(paginas),\n",
    "            'metodo_http': random.choice(metodos),\n",
    "            'codigo_estado': random.choice(status),\n",
    "            'timestamp': fake.date_time_between(start_date=usuario['fecha_registro'], end_date='now')\n",
    "        })\n",
    "    return data\n",
    "\n",
    "print(\"Funciones de generación de datos creadas.\")\n",
    "\n",
    "# --- 3. Creación de DataFrames de Spark ---\n",
    "\n",
    "usuarios_data = generar_usuarios(100)\n",
    "productos_data = generar_productos(50)\n",
    "pedidos_data = generar_pedidos(usuarios_data, productos_data, 500)\n",
    "logs_web_data = generar_logs_web(usuarios_data, 2000)\n",
    "\n",
    "usuarios_df = spark.createDataFrame(usuarios_data)\n",
    "productos_df = spark.createDataFrame(productos_data)\n",
    "pedidos_df = spark.createDataFrame(pedidos_data)\n",
    "logs_web_df = spark.createDataFrame(logs_web_data)\n",
    "\n",
    "print(\"\\n--- DataFrames Relacionales Creados ---\")\n",
    "usuarios_df.show(3)\n",
    "productos_df.show(3)\n",
    "pedidos_df.show(3)\n",
    "logs_web_df.show(3, truncate=False)\n",
    "\n",
    "# --- 4. Generación de Datos No Estructurados (Simulación de Archivos) ---\n",
    "\n",
    "def generar_y_guardar_facturas_texto(pedidos, usuarios, productos, ruta_dbfs):\n",
    "    \"\"\"\n",
    "    Simula la creación de archivos de facturas (como si fueran PDFs convertidos a texto).\n",
    "    Guarda cada factura como un archivo .txt en la ruta de DBFS especificada.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerando archivos de facturas en la ruta: {ruta_dbfs}\")\n",
    "    \n",
    "    # Crear un mapa de usuarios y productos para búsqueda fácil\n",
    "    mapa_usuarios = {u['id_usuario']: u for u in usuarios}\n",
    "    mapa_productos = {p['id_producto']: p for p in productos}\n",
    "    \n",
    "    # Asegurarse de que el directorio existe\n",
    "    dbutils.fs.mkdirs(ruta_dbfs)\n",
    "    \n",
    "    # Tomar una muestra de 50 pedidos para generar facturas\n",
    "    for pedido in random.sample(pedidos, 50):\n",
    "        usuario = mapa_usuarios.get(pedido['id_usuario'])\n",
    "        producto = mapa_productos.get(pedido['id_producto'])\n",
    "        \n",
    "        if not usuario or not producto:\n",
    "            continue\n",
    "            \n",
    "        # Crear el contenido de la factura como un string\n",
    "        contenido_factura = f\"\"\"\n",
    "        ========================================\n",
    "        FACTURA ELECTRÓNICA\n",
    "        ========================================\n",
    "        \n",
    "        Número de Factura: INV-{pedido['id_pedido']}\n",
    "        Fecha: {pedido['fecha_pedido'].strftime('%Y-%m-%d %H:%M:%S')}\n",
    "        \n",
    "        --- Cliente ---\n",
    "        ID Cliente: {usuario['id_usuario']}\n",
    "        Nombre: {usuario['nombre']}\n",
    "        Email: {usuario['email']}\n",
    "        Ciudad: {usuario['ciudad']}\n",
    "        \n",
    "        --- Detalles del Pedido ---\n",
    "        ID Pedido: {pedido['id_pedido']}\n",
    "        \n",
    "        Descripción                 Cantidad      Precio Unit.      Total\n",
    "        -----------------------------------------------------------------\n",
    "        {producto['nombre_producto']:<28}{pedido['cantidad']:<14}${producto['precio_unitario']:<16.2f}${pedido['monto']:.2f}\n",
    "        \n",
    "        ========================================\n",
    "        TOTAL A PAGAR: ${pedido['monto']:.2f}\n",
    "        ========================================\n",
    "        \"\"\"\n",
    "        \n",
    "        # Guardar el string en un archivo en DBFS\n",
    "        nombre_archivo = f\"factura_{pedido['id_pedido']}.txt\"\n",
    "        dbutils.fs.put(f\"{ruta_dbfs}/{nombre_archivo}\", contenido_factura, overwrite=True)\n",
    "        \n",
    "    print(f\"Se generaron 50 archivos de factura de ejemplo en {ruta_dbfs}\")\n",
    "    print(\"Los estudiantes pueden usar Auto Loader o spark.read.text() para ingerir estos datos.\")\n",
    "\n",
    "# Ejecutar la función para generar los archivos de factura\n",
    "#ruta_facturas = \"/tmp/facturas_raw\"\n",
    "#generar_y_guardar_facturas_texto(pedidos_data, usuarios_data, productos_data, ruta_facturas)\n",
    "\n",
    "\n",
    "# --- 5. Función para Generar Documentos de Perfil 360 (Simulación NoSQL) ---\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def crear_perfil_360(usuarios_df, pedidos_df, logs_web_df):\n",
    "    \"\"\"\n",
    "    Combina los DataFrames para crear un perfil 360 de cada cliente (simulación de Cosmos DB/MongoDB).\n",
    "    \"\"\"\n",
    "    pedidos_agrupados = pedidos_df.groupBy(\"id_usuario\").agg(F.collect_list(F.struct(\"id_pedido\", \"id_producto\", \"monto\", \"fecha_pedido\")).alias(\"pedidos\"))\n",
    "    logs_agrupados = logs_web_df.groupBy(\"id_usuario\").agg(F.collect_list(F.struct(\"pagina_visitada\", \"timestamp\")).alias(\"actividad_web\"))\n",
    "    \n",
    "    perfil_360_df = usuarios_df.join(pedidos_agrupados, \"id_usuario\", \"left\").join(logs_agrupados, \"id_usuario\", \"left\")\n",
    "    return perfil_360_df\n",
    "\n",
    "print(\"\\n--- Generando Documentos de Perfil 360 (Simulación NoSQL) ---\")\n",
    "perfil_360_df = crear_perfil_360(usuarios_df, pedidos_df, logs_web_df)\n",
    "\n",
    "print(\"Mostrando una muestra de los perfiles 360:\")\n",
    "perfil_360_df.show(3, truncate=False)\n",
    "\n",
    "# --- 6. Visualización de un Documento JSON ---\n",
    "\n",
    "print(\"\\n--- Ejemplo de un Documento JSON para el Perfil 360 ---\")\n",
    "primer_perfil_json = perfil_360_df.first().asDict(recursive=True)\n",
    "print(json.dumps(primer_perfil_json, indent=4, default=str))\n",
    "\n",
    "#\n",
    "# Fin del Script\n",
    "# Los DataFrames y los archivos de texto ya están listos para el taller.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd77b72a-089b-4b4b-b5f6-0582d20764c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🏗️ Paso 2: Construyendo la Capa de Bronce (Bronze)\n",
    "Ya hemos generado nuestros DataFrames en memoria. Ahora, vamos a dar el primer paso para construir nuestro Lakehouse: persistir los datos crudos.\n",
    "\n",
    "En la metodología de Databricks, la primera capa se conoce como Bronce. Esta capa contiene los datos en su estado más puro, tal como llegan de los sistemas de origen. Es nuestra copia de seguridad y el punto de partida para cualquier pipeline de datos.\n",
    "\n",
    "Acción\n",
    "En la siguiente celda, vamos a guardar cada uno de nuestros DataFrames relacionales (usuarios_df, productos_df, etc.) como una tabla Delta en el catálogo de Databricks. Usaremos el sufijo _bronze para identificar claramente que pertenecen a esta capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0e4434-2075-4c42-94ef-f6f5f3e38151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Celda 2: Guardar DataFrames Relacionales como Tablas Delta (Capa Bronce)\n",
    "#\n",
    "# Objetivo: Persistir los datos crudos que generamos en el paso anterior\n",
    "# en el Unity Catalog (o Hive Metastore) de Databricks.\n",
    "# Usamos el formato Delta Lake por sus ventajas (ACID, Time Travel, etc.).\n",
    "#\n",
    "\n",
    "# --- Definir el nombre de la base de datos (o schema) ---\n",
    "# Es una buena práctica organizar las tablas en un schema.\n",
    "# Asegúrate de que este schema exista o de tener permisos para crearlo.\n",
    "# Si no usas Unity Catalog, puedes omitir el catálogo 'main'.\n",
    "db_name = \"curso_arquitecturas\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "print(f\"Usando la base de datos: {db_name}\")\n",
    "\n",
    "# --- Guardar cada DataFrame como una tabla Delta ---\n",
    "\n",
    "# Guardar la tabla de usuarios\n",
    "usuarios_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"usuarios_bronze\")\n",
    "\n",
    "print(\"Tabla 'usuarios_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de productos\n",
    "productos_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"productos_bronze\")\n",
    "\n",
    "print(\"Tabla 'productos_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de pedidos\n",
    "pedidos_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"pedidos_bronze\")\n",
    "\n",
    "print(\"Tabla 'pedidos_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de logs web\n",
    "logs_web_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"logs_web_bronze\")\n",
    "\n",
    "print(\"Tabla 'logs_web_bronze' guardada exitosamente.\")\n",
    "\n",
    "print(\"\\n¡Proceso completado! Las 4 tablas de la capa Bronce ya están disponibles en el catálogo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd997107-e119-4478-ae89-670fb83f14fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clase _1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
