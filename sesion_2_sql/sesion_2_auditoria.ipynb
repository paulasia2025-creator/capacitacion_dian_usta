{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4641eb06-d3e6-4179-a3bf-03b5e51c2e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Generaci√≥n del Conjunto de Datos de Origen**\n",
    "\n",
    "**Objetivo:** El primer paso en cualquier proyecto de datos es tener datos. En esta celda, simulamos el proceso de recibir datos crudos de los distintos sistemas de una empresa de e-commerce. En lugar de usar un archivo CSV est√°tico, generamos los datos mediante programaci√≥n para crear un escenario din√°mico y realista.\n",
    "\n",
    "---\n",
    "## **Simulando un Ecosistema de Datos Real**\n",
    "\n",
    "En el mundo real, los datos no provienen de un solo lugar. Llegan de m√∫ltiples fuentes:\n",
    "\n",
    "* Un sistema **CRM** para la gesti√≥n de usuarios.\n",
    "* Un **ERP** para el manejo de proveedores y productos.\n",
    "* Una **plataforma de ventas** que registra los pedidos.\n",
    "* Un **sistema de log√≠stica** para gestionar las devoluciones.\n",
    "\n",
    "Este script utiliza Python, PySpark y la librer√≠a **`Faker`** para imitar este ecosistema, creando cinco DataFrames distintos que representan estas fuentes.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## **Componentes Clave del Script**\n",
    "\n",
    "* **`Faker`**: Es una potente librer√≠a de Python que nos permite crear datos falsos pero realistas (nombres, empresas, ciudades, etc.). Es una herramienta fundamental para probar pipelines de datos sin exponer informaci√≥n sensible.\n",
    "\n",
    "* **La Semilla de la Reproducibilidad (`SEED = 42`) üå±**: Este es uno de los conceptos m√°s importantes de la celda. Al fijar una \"semilla\", nos aseguramos de que el generador de n√∫meros \"aleatorios\" produzca **siempre la misma secuencia**. Esto significa que, sin importar cu√°ntas veces ejecutemos el c√≥digo, los datos generados ser√°n id√©nticos. Para un taller, esto es crucial, ya que garantiza que todos los participantes trabajen con el mismo conjunto de datos y obtengan los mismos resultados.\n",
    "\n",
    "* **Las Entidades de Negocio Creadas üìà**: Hemos modelado cinco entidades clave para nuestro negocio simulado, lo que nos dar√° material para an√°lisis interesantes:\n",
    "    * **`Proveedores`**: ¬øDe d√≥nde vienen nuestros productos?\n",
    "    * **`Usuarios`**: ¬øQui√©nes son nuestros clientes?\n",
    "    * **`Productos`**: ¬øQu√© vendemos? (Ahora vinculados a un proveedor).\n",
    "    * **`Pedidos`**: El registro de las ventas, el coraz√≥n de nuestro negocio.\n",
    "    * **`Devoluciones`**: ¬øQu√© productos se devuelven y por qu√©?\n",
    "\n",
    "Al finalizar la ejecuci√≥n de esta celda, tendremos cinco DataFrames de PySpark cargados en la memoria del cl√∫ster, listos para ser procesados e ingeridos en nuestra arquitectura Medallion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a890929-cbbb-4369-a2b9-0500027fbaac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 1: Script para generar un conjunto de datos de e-commerce enriquecido\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- SEMILLA PARA REPRODUCIBILIDAD ---\n",
    "# Garantiza que los datos generados sean siempre los mismos.\n",
    "SEED = 2025\n",
    "Faker.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Inicializar Faker y Spark Session\n",
    "fake = Faker('es_ES')\n",
    "# spark = SparkSession.builder.appName(\"Taller de Arquitecturas de Datos\").getOrCreate() # En Databricks, 'spark' ya est√° disponible\n",
    "\n",
    "# --- 1. GENERACI√ìN DE PROVEEDORES ---\n",
    "def generar_proveedores(n=10):\n",
    "    \"\"\"Genera una lista de proveedores de productos.\"\"\"\n",
    "    return [{'id_proveedor': 500 + i, 'nombre_proveedor': fake.company()} for i in range(n)]\n",
    "\n",
    "# --- 2. GENERACI√ìN DE USUARIOS ---\n",
    "def generar_usuarios(n=100):\n",
    "    \"\"\"Genera una lista de diccionarios de usuarios.\"\"\"\n",
    "    return [{'id_usuario': 1000 + i, 'nombre': fake.name(), 'email': fake.email(), 'fecha_registro': fake.date_time_between(start_date='-2y'), 'ciudad': fake.city()} for i in range(n)]\n",
    "\n",
    "# --- 3. GENERACI√ìN DE PRODUCTOS (Ahora con proveedor) ---\n",
    "def generar_productos(proveedores, n=50):\n",
    "    \"\"\"Genera una lista de productos, asignando un proveedor a cada uno.\"\"\"\n",
    "    categorias = ['Electr√≥nica', 'Hogar', 'Ropa', 'Libros', 'Deportes']\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        data.append({\n",
    "            'id_producto': 2000 + i,\n",
    "            'id_proveedor': random.choice(proveedores)['id_proveedor'],\n",
    "            'nombre_producto': fake.word().capitalize() + \" \" + fake.word(),\n",
    "            'categoria': random.choice(categorias),\n",
    "            'precio_unitario': round(random.uniform(5.0, 250.0), 2)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# --- 4. GENERACI√ìN DE PEDIDOS (Ahora con m√©todo de pago) ---\n",
    "def generar_pedidos(usuarios, productos, n=500):\n",
    "    \"\"\"Genera una lista de pedidos, vinculando usuarios y productos.\"\"\"\n",
    "    metodos_pago = ['Tarjeta de Cr√©dito', 'PayPal', 'PSE', 'Efectivo']\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        usuario = random.choice(usuarios)\n",
    "        producto = random.choice(productos)\n",
    "        cantidad = random.randint(1, 5)\n",
    "        data.append({\n",
    "            'id_pedido': 3000 + i,\n",
    "            'id_usuario': usuario['id_usuario'],\n",
    "            'id_producto': producto['id_producto'],\n",
    "            'cantidad': cantidad,\n",
    "            'monto': round(cantidad * producto['precio_unitario'], 2),\n",
    "            'metodo_pago': random.choice(metodos_pago),\n",
    "            'fecha_pedido': fake.date_time_between(start_date=usuario['fecha_registro'])\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# --- 5. GENERACI√ìN DE DEVOLUCIONES ---\n",
    "def generar_devoluciones(pedidos, n=30):\n",
    "    \"\"\"Genera una lista de devoluciones, seleccionando pedidos al azar.\"\"\"\n",
    "    motivos = ['Producto defectuoso', 'No era la talla correcta', 'No cumple expectativas', 'Me arrepent√≠ de la compra']\n",
    "    pedidos_devueltos = random.sample(pedidos, n)\n",
    "    data = []\n",
    "    for i, pedido in enumerate(pedidos_devueltos):\n",
    "        data.append({\n",
    "            'id_devolucion': 7000 + i,\n",
    "            'id_pedido': pedido['id_pedido'],\n",
    "            'fecha_devolucion': fake.date_time_between(start_date=pedido['fecha_pedido']),\n",
    "            'motivo': random.choice(motivos)\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# --- CREACI√ìN DE DATAFRAMES EN MEMORIA ---\n",
    "print(\"Generando datos simulados...\")\n",
    "proveedores_data = generar_proveedores(500)\n",
    "usuarios_data = generar_usuarios(15000)\n",
    "productos_data = generar_productos(proveedores_data, 10000)\n",
    "pedidos_data = generar_pedidos(usuarios_data, productos_data, 5000000)\n",
    "devoluciones_data = generar_devoluciones(pedidos_data, 300000)\n",
    "\n",
    "proveedores_df = spark.createDataFrame(proveedores_data)\n",
    "usuarios_df = spark.createDataFrame(usuarios_data)\n",
    "productos_df = spark.createDataFrame(productos_data)\n",
    "pedidos_df = spark.createDataFrame(pedidos_data)\n",
    "devoluciones_df = spark.createDataFrame(devoluciones_data)\n",
    "\n",
    "print(\"\\n--- DataFrames Creados en Memoria ---\")\n",
    "print(f\"Proveedores: {proveedores_df.count()} registros\")\n",
    "print(f\"Usuarios: {usuarios_df.count()} registros\")\n",
    "print(f\"Productos: {productos_df.count()} registros\")\n",
    "print(f\"Pedidos: {pedidos_df.count()} registros\")\n",
    "print(f\"Devoluciones: {devoluciones_df.count()} registros\")\n",
    "\n",
    "print(\"\\nCelda 1 completada: El conjunto de datos enriquecido est√° listo para ser ingerido en la capa Bronce.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de720ecf-6291-40a9-9418-04a675c2b0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingesta a la Capa BRONCE - Persistencia de los Datos de Origen\n",
    "\n",
    "**Objetivo:** El prop√≥sito de esta celda es tomar los datos que existen temporalmente en la memoria del cl√∫ster (los DataFrames de la Celda 1) y guardarlos como tablas permanentes. Este es el primer paso oficial de nuestro pipeline: la materializaci√≥n de los datos en la capa **Bronce**.\n",
    "\n",
    "---\n",
    "## **El Archivo de Datos Crudos**\n",
    "\n",
    "La capa Bronce funciona como el **archivo hist√≥rico y fiel** de nuestros datos de origen. Su principal y √∫nica regla es almacenar la informaci√≥n **exactamente como fue recibida**, sin aplicar ninguna limpieza, transformaci√≥n o validaci√≥n.\n",
    "\n",
    "Este enfoque nos proporciona dos ventajas estrat√©gicas:\n",
    "\n",
    "1.  **Auditabilidad y Linaje**: Mantenemos un registro perfecto del estado de los datos en el momento de su ingesta. Esto es crucial para auditor√≠as y para rastrear el origen de cualquier dato en las capas posteriores.\n",
    "2.  **Capacidad de Reprocesamiento**: Si en el futuro las reglas de negocio cambian, siempre podemos volver a esta capa Bronce intacta y volver a ejecutar nuestros pipelines de transformaci√≥n desde el principio con la nueva l√≥gica, sin tener que volver a conectarnos a los sistemas de origen.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## **Componentes Clave del Script**\n",
    "\n",
    "* **Formato Delta Lake**: No guardamos los datos en un formato simple como CSV. Usamos **Delta Lake**, el est√°ndar en Databricks. Esto convierte nuestro almacenamiento en un sistema transaccional robusto, d√°ndonos garant√≠as **ACID** (los trabajos no quedan a medias), un rendimiento optimizado y la capacidad de \"viajar en el tiempo\" a versiones anteriores de los datos.\n",
    "\n",
    "* **`saveAsTable()`**: Este comando es el coraz√≥n de la celda. Realiza dos acciones clave:\n",
    "    1.  **Guarda los datos** en el almacenamiento subyacente (como archivos Parquet optimizados).\n",
    "    2.  **Registra la tabla** en el cat√°logo de Databricks (Unity Catalog o Hive Metastore), lo que la hace visible y consultable mediante SQL.\n",
    "\n",
    "* **Organizaci√≥n (`USE curso_arquitecturas`)**: Antes de guardar, nos aseguramos de estar en la base de datos correcta. Mantener las tablas de un proyecto dentro de su propio `schema` o base de datos es una pr√°ctica esencial para tener un espacio de trabajo ordenado.\n",
    "\n",
    "Al finalizar la ejecuci√≥n de esta celda, nuestros datos ya no son temporales. Se han convertido en cinco tablas Delta persistentes que forman la base de nuestra arquitectura, listas para ser refinadas en la siguiente capa: Plata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d47329-971a-4dc3-a842-7128c4890a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Guardar los DataFrames como Tablas Delta en la Capa Bronce\n",
    "\n",
    "# --- 1. Definir y usar la base de datos para nuestro proyecto ---\n",
    "db_name = \"curso_arquitecturas\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "print(f\"Usando la base de datos: {db_name}\\n\")\n",
    "\n",
    "# --- 2. Persistir cada DataFrame como una tabla Delta en la capa Bronce ---\n",
    "\n",
    "# Guardar la tabla de proveedores\n",
    "proveedores_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"proveedores_bronze\")\n",
    "print(\"Tabla 'proveedores_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de usuarios\n",
    "usuarios_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"usuarios_bronze\")\n",
    "print(\"Tabla 'usuarios_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de productos\n",
    "productos_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .saveAsTable(\"productos_bronze\")\n",
    "print(\"Tabla 'productos_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de pedidos\n",
    "pedidos_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .saveAsTable(\"pedidos_bronze\")\n",
    "print(\"Tabla 'pedidos_bronze' guardada exitosamente.\")\n",
    "\n",
    "# Guardar la tabla de devoluciones\n",
    "devoluciones_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"devoluciones_bronze\")\n",
    "print(\"Tabla 'devoluciones_bronze' guardada exitosamente.\")\n",
    "\n",
    "print(\"\\n¬°Proceso completado! Las 5 tablas de la capa Bronce ya est√°n disponibles en el cat√°logo para la siguiente etapa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4648e81b-4e1f-4aaf-bf33-bfa0b39116c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Celda 2.5 (Explicativa): Propiedades ACID y Auditor√≠a de Datos en Delta Lake**\n",
    "\n",
    "Antes de proceder con la transformaci√≥n de datos a la capa Plata, es importante comprender una caracter√≠stica fundamental de las tablas Delta que acabamos de crear: las garant√≠as transaccionales **ACID**.\n",
    "\n",
    "-----\n",
    "\n",
    "## **Garant√≠as Transaccionales ACID**\n",
    "\n",
    "ACID es un acr√≥nimo que designa un conjunto de cuatro propiedades que aseguran la fiabilidad e integridad de los datos durante las operaciones de escritura.\n",
    "\n",
    "  * **Atomicidad (A)**: Asegura que cada transacci√≥n se trate como una √∫nica unidad de trabajo que se ejecuta completamente o no se ejecuta en absoluto. Su principal beneficio es la prevenci√≥n de la corrupci√≥n de datos por escrituras parciales.\n",
    "\n",
    "  * **Consistencia (C)**: Garantiza que cada transacci√≥n lleva los datos de un estado v√°lido a otro. Se preserva la integridad de la base de datos seg√∫n las reglas definidas.\n",
    "\n",
    "  * **Aislamiento (I)**: Asegura que las transacciones concurrentes se ejecuten de manera independiente, sin interferir entre s√≠. Esto previene inconsistencias al leer datos que est√°n siendo modificados simult√°neamente.\n",
    "\n",
    "  * **Durabilidad (D)**: Una vez que una transacci√≥n se ha completado, sus cambios son permanentes y persistir√°n incluso en caso de fallos del sistema.\n",
    "\n",
    "En conjunto, estas propiedades otorgan al Data Lake la fiabilidad de los sistemas de bases de datos tradicionales, lo cual es indispensable en entornos de producci√≥n.\n",
    "\n",
    "-----\n",
    "\n",
    "## **Auditor√≠a y Versionado de Datos**\n",
    "\n",
    "Estas garant√≠as son posibles gracias al **registro de transacciones** (transaction log) que Delta Lake mantiene para cada tabla. Este registro es consultable y proporciona una completa auditor√≠a de todas las operaciones realizadas.\n",
    "\n",
    "El comando `DESCRIBE HISTORY` permite inspeccionar este historial.\n",
    "\n",
    "**C√≥digo:**\n",
    "\n",
    "```sql\n",
    "-- Consultar el historial de transacciones de la tabla de pedidos.\n",
    "DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze;\n",
    "```\n",
    "\n",
    "**An√°lisis del Resultado:**\n",
    "\n",
    "La salida de este comando detalla cada operaci√≥n (versi√≥n, marca de tiempo, tipo de operaci√≥n, par√°metros y usuario) que ha modificado la tabla.\n",
    "\n",
    "Esta capacidad de versionado es la base para la funcionalidad de \"Time Travel\", que permite consultar estados anteriores de los datos para fines de depuraci√≥n, auditor√≠a o restauraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ab0d95-f8db-4267-b110-1147c5c0816b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"operation\":255,\"userId\":229},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755182124270}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef916b88-3476-4746-9afb-543b95b75b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 1: Viaje en el Tiempo por N√∫mero de Versi√≥n (`VERSION AS OF`)**\n",
    "\n",
    "La forma m√°s directa de viajar en el tiempo es pidiendo una versi√≥n espec√≠fica. Como nuestra tabla `pedidos_bronze` se cre√≥ en una sola operaci√≥n (`WRITE`), solo tiene la versi√≥n 0 y la 1 (la creaci√≥n y la escritura). Vamos a simular un cambio para poder comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6202cef-2bf9-48e6-a021-c3a4c4ce6a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Primero, vamos a simular un cambio: borremos los pedidos del m√©todo de pago 'Efectivo'.\n",
    "-- Esto crear√° la versi√≥n 2 de la tabla.\n",
    "DELETE FROM curso_arquitecturas.pedidos_bronze WHERE metodo_pago = 'Efectivo';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0f8e66-e71c-44d9-95d6-f1a349fe2a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM curso_arquitecturas.pedidos_bronze \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65746953-b935-4e8e-90b0-8ed159fe65ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY pedidos_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154f2f67-033d-49ff-ac70-29168fdb52c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "--- versi√≥n AS OF\n",
    "SELECT \n",
    " *\n",
    "FROM curso_arquitecturas.pedidos_bronze VERSION AS OF 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2df71d7-bfa9-44d0-a20d-00d2b6a3ff35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ahora, comparemos la versi√≥n m√°s reciente (2) con la anterior (1).\n",
    "SELECT \n",
    "  'Version 1 (Antes del borrado)' AS version, \n",
    "  COUNT(*) AS total_filas \n",
    "FROM curso_arquitecturas.pedidos_bronze VERSION AS OF 19\n",
    "UNION ALL\n",
    "SELECT \n",
    "  'Version 2 (Despu√©s del borrado)' AS version, \n",
    "  COUNT(*) AS total_filas \n",
    "FROM curso_arquitecturas.pedidos_bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0727a89-1764-4e4b-9345-e0bd4a73d54b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"userId\":170},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755182471877}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef35f6f6-1031-45f6-af1e-7b9251c68e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Caso de uso:** Perfecto para restaurar una tabla a un estado anterior despu√©s de un error, como un borrado accidental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04425bf4-384a-4b3b-aa51-2fdd8f5c4932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 2: Viaje en el Tiempo por Marca de Tiempo (`TIMESTAMP AS OF`)**\n",
    "\n",
    "A veces no conoces el n√∫mero de la versi√≥n, pero s√≠ sabes la hora en que los datos estaban correctos. Puedes usar una marca de tiempo para consultar. La cadena de fecha y hora debe estar en el formato est√°ndar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5538c35-78d8-4297-aebd-a74e3ec2f116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Consulta el estado de la tabla de usuarios ANTES de que ejecut√°ramos este notebook hoy.\n",
    "-- Reemplaza con una marca de tiempo anterior a la ejecuci√≥n de la celda 2.\n",
    "SELECT \n",
    "  fecha_registro,\n",
    "  nombre\n",
    "FROM curso_arquitecturas.usuarios_bronze TIMESTAMP AS OF \"2025-08-13T09:45:00.000+0000\" -- Ajusta la fecha y hora a tu zona si es necesario\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f8bf92-1295-4938-bd43-c678f3174edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Caso de uso:** Investigar el estado de los datos justo antes de un despliegue o una ejecuci√≥n de pipeline que pudo haber causado un problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b973395c-5d66-4759-bc2c-98801a4f54cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 3: Ver los Cambios Entre Dos Versiones (`table_changes`)**\n",
    "\n",
    "Esta es una funci√≥n de tabla incre√≠blemente √∫til para la auditor√≠a. Te muestra exactamente qu√© filas cambiaron entre dos versiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a387aee8-6ba6-4c76-a378-c1871ab75455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Paso 1: Habilitar Change Data Feed con la sintaxis correcta.**\n",
    "-- El nombre de la propiedad es 'delta.enableChangeDataFeed' (sin guion).\n",
    "ALTER TABLE curso_arquitecturas.pedidos_bronze SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d1e3d0-064d-43a9-9770-7278f5520ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Paso 2: Realizar una nueva operaci√≥n para generar un cambio que sea rastreado.**\n",
    "-- CDF solo rastrea los cambios OCURRIDOS DESPU√âS de su habilitaci√≥n.\n",
    "-- Vamos a borrar los pedidos pagados con 'PayPal' para generar un nuevo historial.\n",
    "DELETE FROM curso_arquitecturas.pedidos_bronze WHERE metodo_pago = 'PayPal';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc95e20-cd95-400c-9c63-efedc842a817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Paso 3: (Opcional pero recomendado) Consultar el historial para obtener los n√∫meros de versi√≥n.**\n",
    "DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze;\n",
    "-- Anota la versi√≥n de la operaci√≥n 'DELETE' y la versi√≥n inmediatamente anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c0ac272-d46f-41b1-988f-aabc2f172fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- **Paso 4: Ejecutar 'table_changes' correctamente.**\n",
    "-- Ahora s√≠ podemos ver las filas exactas que se eliminaron en la √∫ltima operaci√≥n.\n",
    "-- Reemplace 'X' con la versi√≥n ANTERIOR al DELETE y 'Y' con la versi√≥n DEL DELETE.\n",
    "SELECT \n",
    "  *, \n",
    "  _commit_version as version_modificacion, \n",
    "  _commit_timestamp as fecha_modificacion \n",
    "FROM table_changes('curso_arquitecturas.pedidos_bronze', 22,24 ); -- Ejemplo: table_changes('...', 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d2c269-c7b9-4153-8e72-23505f72dfa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Caso de uso:** Auditor√≠a fina. Identificar qu√© datos espec√≠ficos fueron alterados por una operaci√≥n, por ejemplo, para un reporte de cumplimiento de normativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4bc1d6c-405d-4448-8ef2-88e82ac78bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 4: Restaurar una Tabla a una Versi√≥n Anterior (`RESTORE`)**\n",
    "\n",
    "Si cometiste un error, no necesitas escribir c√≥digo complejo para arreglarlo. `RESTORE` revierte la tabla a una versi√≥n anterior, creando una nueva transacci√≥n de restauraci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d0a6db-63f0-4f3e-a6fc-eb078b48204f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ¬°Vamos a deshacer el borrado que hicimos en el Ejemplo 1!\n",
    "-- Esto restaurar√° la tabla al estado de la versi√≥n 0.\n",
    "RESTORE TABLE curso_arquitecturas.pedidos_bronze TO VERSION AS OF 34;\n",
    "\n",
    "-- Verifiquemos el historial para ver la nueva entrada de 'RESTORE'.\n",
    "DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ce7cd2-f5ba-4bc2-b50f-143d07557c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Caso de uso:** Recuperaci√≥n de desastres de forma r√°pida y segura. Es la forma oficial de \"deshacer\" una mala operaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73781018-60b5-4e49-94eb-af164fb33d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 5: Auditor√≠a de Operaciones a Nivel de Archivo**\n",
    "\n",
    "El historial tambi√©n nos dice qu√© archivos de datos (Parquet) fueron a√±adidos o eliminados en cada transacci√≥n. Esto es √∫til para una depuraci√≥n a bajo nivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6371e0d-2a38-4335-9bcf-25bedaba277f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Miremos el historial de la tabla de devoluciones y veamos las m√©tricas de la operaci√≥n de escritura.\n",
    "SELECT \n",
    "  version, \n",
    "  timestamp,\n",
    "  operation,\n",
    "  operationMetrics.numFiles AS archivos_escritos,\n",
    "  operationMetrics.numOutputRows AS filas_escritas,\n",
    "  operationMetrics.numOutputBytes AS bytes_escritos,\n",
    "  operationMetrics.numDeletedRows AS filas_borradas\n",
    "FROM (DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd54338-9549-4b59-8006-56153656bbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Caso de uso:** Optimizaci√≥n de rendimiento. Si ves que una operaci√≥n est√° generando miles de archivos peque√±os, es una se√±al de que necesitas optimizar la escritura (con t√©cnicas como `OPTIMIZE` o ajustando el tama√±o de los archivos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa554557-9f87-487c-ac03-787526ff0917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 6: Auditor√≠a Forense - Identificar el Notebook o Job que Modific√≥ los Datos**\n",
    "\n",
    "**Caso de uso:** En un entorno colaborativo, m√∫ltiples procesos y usuarios pueden modificar una tabla. Si ocurre un error, no basta con saber *qui√©n* hizo el cambio, sino *desde d√≥nde* (qu√© notebook o qu√© job automatizado). Esta informaci√≥n est√° anidada en el historial.\n",
    "\n",
    "**C√≥digo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86374bd1-307b-45ee-bc64-5682dd908420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Extraemos detalles del contexto de la ejecuci√≥n de cada transacci√≥n.\n",
    "-- 'notebook.notebookId' es extremadamente √∫til para la depuraci√≥n.\n",
    "SELECT \n",
    "  version,\n",
    "  timestamp,\n",
    "  userName AS usuario,\n",
    "  operation,\n",
    "  operationParameters.mode,\n",
    "  notebook.notebookId AS id_del_notebook,\n",
    "  clusterId as id_del_cluster\n",
    "FROM \n",
    "  (DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf26443a-5f1d-4fd8-a77b-3a6c1b9f3c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**An√°lisis del Resultado:**\n",
    "Esta consulta enriquece el historial est√°ndar. La columna `id_del_notebook` te da un enlace directo al notebook que ejecut√≥ la transacci√≥n, permiti√©ndote ir directamente al c√≥digo fuente que caus√≥ el cambio. Es una de las herramientas de depuraci√≥n m√°s potentes en Databricks.\n",
    "\n",
    "---\n",
    "### **Interpretaci√≥n del Historial de la Tabla**\n",
    "\n",
    "Aqu√≠ se desglosa la secuencia de eventos que ocurrieron, narrando la historia de la tabla desde su creaci√≥n hasta su estado m√°s reciente.\n",
    "\n",
    "* **Versi√≥n 0 (12 de Agosto): Creaci√≥n Inicial**\n",
    "    La tabla fue creada por primera vez. Este fue el punto de partida.\n",
    "\n",
    "* **Versi√≥n 1 (14 de Agosto): Reemplazo Total de la Tabla**\n",
    "    Dos d√≠as despu√©s, la tabla fue completamente reemplazada usando `CREATE OR REPLACE TABLE`. Esto significa que todo el contenido de la versi√≥n 0 fue descartado, y la tabla comenz√≥ de nuevo. Este evento marca el inicio de la sesi√≥n de trabajo principal.\n",
    "\n",
    "* **Versiones 2 y 3: Modificaci√≥n y Mantenimiento**\n",
    "    Inmediatamente despu√©s del reemplazo, se realiz√≥ una operaci√≥n de borrado (`DELETE`). Justo despu√©s, se ejecut√≥ un comando `OPTIMIZE`, que es una operaci√≥n de mantenimiento para compactar archivos peque√±os y mejorar el rendimiento de lectura.\n",
    "\n",
    "* **Versiones 4 y 5: Habilitaci√≥n de CDF y Nuevo Borrado**\n",
    "    La operaci√≥n `SET TBLPROPERTIES` probablemente fue para habilitar el **Change Data Feed (CDF)** (`delta.enableChangeDataFeed`). Inmediatamente despu√©s, se realiz√≥ otro `DELETE`, seguramente para probar la funcionalidad de `table_changes` que depende de CDF.\n",
    "\n",
    "* **Versiones 6, 7 y 8: Ciclo Adicional de Mantenimiento y Pruebas**\n",
    "    El patr√≥n se repite: una optimizaci√≥n (`OPTIMIZE`), seguida de otra modificaci√≥n de propiedades y un `DELETE` final. Esto refuerza la idea de que se estaba llevando a cabo una sesi√≥n de pruebas intensiva.\n",
    "\n",
    "* **Versiones 9 y 10: Recuperaci√≥n de Datos (`RESTORE`)**\n",
    "    Estos son los eventos m√°s significativos. Las dos operaciones `RESTORE` consecutivas indican que el usuario revirti√≥ la tabla a un estado anterior, deshaciendo efectivamente uno o m√°s de los borrados anteriores. Esto confirma que las operaciones `DELETE` eran parte de un experimento o un error que se necesitaba corregir.\n",
    "\n",
    "---\n",
    "### **Observaciones Clave üî¨**\n",
    "\n",
    "1.  **Ausencia del `id_del_notebook`**: El hecho de que `id_del_notebook` sea `null` en todas las operaciones es una pista muy importante. Generalmente, esto significa que los comandos SQL no se ejecutaron desde un notebook de Databricks (con un cl√∫ster de computaci√≥n), sino directamente a trav√©s del **Editor de SQL de Databricks** conectado a un **SQL Warehouse**.\n",
    "\n",
    "2.  **Sesi√≥n de Pruebas Intensiva**: La r√°pida sucesi√≥n de operaciones variadas (`DELETE`, `OPTIMIZE`, `SET TBLPROPERTIES`, `RESTORE`) en un corto per√≠odo de tiempo (la mayor√≠a en menos de 30 minutos el 14 de agosto) es un claro indicador de una sesi√≥n de desarrollo o depuraci√≥n, no de un pipeline de producci√≥n automatizado.\n",
    "\n",
    "3.  **Uso de Funcionalidades Avanzadas**: El historial demuestra un uso pr√°ctico de las caracter√≠sticas que hacen poderoso a Delta Lake: la capacidad de modificar, optimizar, habilitar funciones y, lo m√°s importante, restaurar la tabla a un punto anterior en el tiempo de forma segura.\n",
    "\n",
    "El historial narra lo que el ingeniero de datos hizo desde el 14 de agosto de 2025, someti√≥ la tabla a una serie de pruebas rigurosas para validar o experimentar con las capacidades de modificaci√≥n y recuperaci√≥n de Delta Lake, probablemente trabajando desde el entorno de Databricks SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dab5011a-0db4-40e4-aade-e0ad2bdcb398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 7: An√°lisis Comparativo - Ver el \"Antes y Despu√©s\" de una Fila**\n",
    "\n",
    "**Caso de uso:** Se ha ejecutado una operaci√≥n `MERGE` para actualizar precios o estados, y necesitas validar que los cambios se aplicaron correctamente, comparando el valor antiguo con el nuevo para filas espec√≠ficas.\n",
    "\n",
    "**C√≥digo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e745b7-2a3e-4061-8b49-e85344836173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 1: Simular una actualizaci√≥n en la tabla de productos.\n",
    "-- Actualizaremos el precio de un producto y la categor√≠a de otro.\n",
    "UPDATE curso_arquitecturas.productos_bronze \n",
    "SET precio_unitario = 99.99 \n",
    "WHERE id_producto = 2001;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e138da-1a5f-4597-bad1-9c77293e341a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 2: Obtener la √∫ltima versi√≥n del historial para comparar.\n",
    "-- Asumamos que la operaci√≥n UPDATE fue la versi√≥n 2. La versi√≥n anterior era la 1.\n",
    "\n",
    "DESCRIBE HISTORY curso_arquitecturas.productos_bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b24f4a-b39f-4796-9c71-70bfde4f84d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 3: Comparar los datos de la misma tabla en dos momentos del tiempo.\n",
    "SELECT\n",
    "  antes.id_producto,\n",
    "  antes.nombre_producto,\n",
    "  antes.precio_unitario AS precio_anterior,\n",
    "  despues.precio_unitario AS precio_nuevo,\n",
    "  antes.categoria AS categoria_anterior,\n",
    "  despues.categoria AS categoria_nueva\n",
    "FROM \n",
    "  curso_arquitecturas.productos_bronze VERSION AS OF 6 AS antes\n",
    "INNER JOIN \n",
    "  curso_arquitecturas.productos_bronze AS despues \n",
    "  ON antes.id_producto = despues.id_producto\n",
    "WHERE \n",
    "  antes.precio_unitario <> despues.precio_unitario OR antes.categoria <> despues.categoria;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a88c9cbc-3cf0-4247-9802-9610618426bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**An√°lisis del Resultado:**\n",
    "Esta consulta te mostrar√° √∫nicamente las filas que han sufrido una modificaci√≥n entre las dos versiones, presentando el valor antiguo y el nuevo en la misma l√≠nea. Es un m√©todo extremadamente eficaz para validar la l√≥gica de negocio en operaciones de `UPDATE` o `MERGE`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94639489-1eb4-4a50-8ea0-2d905f897a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Ejemplo 8: Creaci√≥n de un Log de Auditor√≠a Permanente**\n",
    "\n",
    "**Caso de uso:** El resultado de `DESCRIBE HISTORY` es una consulta temporal. Para un cumplimiento normativo o una auditor√≠a a largo plazo, necesitas almacenar este historial de forma permanente en su propia tabla Delta.\n",
    "\n",
    "**C√≥digo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc13223-c2d3-4f06-acac-c6ddb4f0fd17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usamos PySpark para mayor flexibilidad al manejar el DataFrame del historial.\n",
    "\n",
    "# Paso 1: Ejecutar DESCRIBE HISTORY y cargarlo en un DataFrame.\n",
    "history_df = spark.sql(\"DESCRIBE HISTORY curso_arquitecturas.pedidos_bronze\")\n",
    "\n",
    "# Paso 2: Seleccionar y aplanar las columnas que nos interesan.\n",
    "audit_log_df = history_df.select(\n",
    "    \"version\",\n",
    "    \"timestamp\",\n",
    "    \"userId\",\n",
    "    \"userName\",\n",
    "    \"operation\",\n",
    "    \"operationParameters\",\n",
    "    \"notebook.notebookId\"\n",
    ")\n",
    "\n",
    "# Paso 3: Guardar este log en una nueva tabla Delta.\n",
    "# Usamos el modo 'append' para poder ejecutar esto peri√≥dicamente y a√±adir el nuevo historial.\n",
    "audit_log_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"log_auditoria_pedidos\")\n",
    "\n",
    "print(\"Log de auditor√≠a guardado exitosamente.\")\n",
    "\n",
    "# Consultar el log permanente.\n",
    "display(spark.sql(\"SELECT * FROM log_auditoria_pedidos ORDER BY version DESC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06eb7a6-9dd6-4c43-9b4a-4700c4f283f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**An√°lisis del Resultado:**\n",
    "Has creado una tabla `log_auditoria_pedidos` que acumula el historial de cambios de tu tabla principal. Puedes construir alertas o informes sobre esta tabla para monitorear actividades sospechosas o simplemente para tener un registro inmutable de todas las transacciones a lo largo del tiempo."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "faker"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7841278766261256,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sesion_2_auditoria",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
