{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9451d313-0979-4986-8c36-c6989c4e44b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Quality: Confianza en tus Datos\n",
    "\n",
    "La Calidad de Datos es el proceso de asegurar que los datos sean **precisos, consistentes, completos y fiables** a lo largo de todo su ciclo de vida. Es el sistema inmunitario de tu Lakehouse, protegiéndolo del famoso principio: **\"Basura entra, basura sale\" (Garbage In, Garbage Out)**.\n",
    "\n",
    "Sin un framework de Data Quality, podrías estar tomando decisiones de negocio críticas basadas en datos incorrectos, duplicados o incompletos, lo que erosionaría por completo la confianza en tu plataforma de datos.\n",
    "\n",
    "-----\n",
    "\n",
    "### ¿Dónde se Aplican las Reglas de Calidad?\n",
    "\n",
    "El punto más crítico para aplicar las reglas de calidad es en la transición de la **capa Bronce a la Plata**. La capa Bronce es el \"salvaje oeste\"; acepta todo tal como viene. La capa Plata, en cambio, es nuestra \"ciudadela de la confianza\". El pipeline que pasa datos de Bronce a Plata actúa como el **guardián en la puerta**, inspeccionando cada registro antes de dejarlo entrar.\n",
    "\n",
    "-----\n",
    "\n",
    "### Las Dimensiones de la Calidad de Datos\n",
    "\n",
    "\"Calidad\" es un término abstracto. En la práctica, lo medimos a través de varias dimensiones. Aquí las más comunes aplicadas a nuestro dataset de Olist:\n",
    "\n",
    "| Dimensión | Pregunta que Responde | Ejemplo en Nuestro Proyecto |\n",
    "| :--- | :--- | :--- |\n",
    "| **Completitud** | ¿Están presentes todos los datos que necesito? | ¿Hay algún pedido en la tabla `orders` que tenga un `customer_id` nulo? (¡No debería\\!) |\n",
    "| **Unicidad** | ¿Hay registros duplicados? | ¿Existe el mismo `product_id` más de una vez en nuestra `dim_productos`? (¡No debería\\!) |\n",
    "| **Validez** | ¿Los datos se ajustan al formato o a las reglas de negocio? | ¿El `order_status` es uno de los valores permitidos ('delivered', 'shipped', etc.)? ¿Hay algún `precio` negativo? |\n",
    "| **Consistencia** | ¿Mis datos son coherentes entre diferentes tablas? | ¿Todos los `customer_id` en la tabla `fact_pedidos` existen en `dim_clientes`? (Las llaves foráneas nos ayudan con esto). |\n",
    "| **Precisión** | ¿El dato es correcto y refleja la realidad? | ¿La suma de los `payment_value` de un pedido coincide con la suma de `price` + `freight_value` de sus items? |\n",
    "\n",
    "-----\n",
    "\n",
    "###  ¿Cómo Implementarlo en Databricks?\n",
    "\n",
    "Databricks ofrece una solución muy potente y moderna para esto llamada **Delta Live Tables (DLT)** con **Expectations**.\n",
    "\n",
    "**¿Qué son las \"Expectations\"?**\n",
    "Son reglas o \"contratos\" que tú defines sobre tus datos. Le dices a DLT qué esperas que sea verdad, y DLT se encarga de validar los datos y tomar acciones.\n",
    "\n",
    "**Ejemplo de cómo se vería en un pipeline de DLT:**\n",
    "\n",
    "```sql\n",
    "-- En un notebook de Delta Live Tables\n",
    "CREATE STREAMING LIVE TABLE customers_silver (\n",
    "  CONSTRAINT customer_id_not_null EXPECT (customer_id IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_state EXPECT (customer_state IN ('SP', 'RJ', 'MG', ...)) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS SELECT * FROM STREAM(sesion_5.bronze.customers_raw);\n",
    "```\n",
    "\n",
    "En este ejemplo:\n",
    "\n",
    "  * `EXPECT (customer_id IS NOT NULL) ON VIOLATION DROP ROW`: Si un cliente llega con un ID nulo, esa fila se descarta y no llega a la capa Plata.\n",
    "  * `EXPECT (customer_state IN ('SP', ...)) ON VIOLATION FAIL UPDATE`: Si llega un cliente con un estado inválido, el pipeline se detiene por completo y alerta a los desarrolladores para que investiguen el problema.\n",
    "\n",
    "### Añadiendo un Paso de Calidad a Nuestro Notebook Actual\n",
    "\n",
    "Aunque DLT es la mejor herramienta, podemos simular un paso de validación simple en nuestro notebook actual para entender el concepto. Podríamos añadir una fase nueva después de la exploración (EDA) y antes de la transformación a Silver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e80f18-d498-42d3-8724-00fc6f54d2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 3.5: Implementación de Guardianes de Calidad (Data Quality Gates)\n",
    "\n",
    "**Objetivo:** Validar la calidad de nuestros datos de la capa Bronce antes de que pasen a la Plata. Si los datos no cumplen con nuestras reglas de negocio, detendremos el pipeline para evitar la corrupción de datos.\n",
    "\n",
    "Para lograr esto, crearemos una función de ayuda en Python que:\n",
    "1.  Ejecuta una consulta SQL para contar el número de \"filas malas\" según una regla.\n",
    "2.  Si el conteo es mayor que cero, lanza una excepción con un mensaje claro, lo que detendrá la ejecución del notebook y hará que el Job falle.\n",
    "3.  Si el conteo es cero, imprime un mensaje de éxito.\n",
    "\n",
    "Este es nuestro \"guardián\" automatizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f595ba-3edc-4f5f-8e12-909eac62fee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE sesion_5.bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3e7dc1-95cf-4e81-95c1-01a6d686c639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Definir la configuración usando nuestra nueva estructura de catálogo\n",
    "catalog_name = \"sesion_5\"\n",
    "bronze_schema = \"bronze\"\n",
    "silver_schema = \"silver\"\n",
    "\n",
    "# 2. Definir la ruta al volumen de la capa Bronze\n",
    "bronze_volume_path = f\"/Volumes/{catalog_name}/{bronze_schema}/raw_files\"\n",
    "\n",
    "# 3. Cargar todos los datasets desde el volumen Bronze en DataFrames\n",
    "print(f\"Leyendo archivos desde: {bronze_volume_path}...\")\n",
    "options = {\"header\": \"true\", \"inferSchema\": \"true\"}\n",
    "\n",
    "customers_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_customers_dataset.csv\")\n",
    "order_items_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_order_items_dataset.csv\")\n",
    "order_payments_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_order_payments_dataset.csv\")\n",
    "orders_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_orders_dataset.csv\")\n",
    "\n",
    "print(\"\\n¡Carga completada! Los DataFrames de la capa Bronze están en memoria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9bbc434-a177-4e5a-aade-6ca9a4e471de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Registrar todos los DataFrames relacionados con los pedidos como vistas temporales\n",
    "orders_df.createOrReplaceTempView(\"orders_bronze_vw\")\n",
    "order_items_df.createOrReplaceTempView(\"order_items_bronze_vw\")\n",
    "order_payments_df.createOrReplaceTempView(\"order_payments_bronze_vw\")\n",
    "customers_df.createOrReplaceTempView(\"customers_bronze_vw\")\n",
    "\n",
    "print(\"Vistas temporales para pedidos, items y pagos creadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eef7c56-5899-4ad9-95c7-61492002425f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Chequeo 1: Verificar que no haya pedidos sin cliente (Completitud)\n",
    "SELECT\n",
    "  COUNT(*) AS pedidos_sin_cliente\n",
    "FROM orders_bronze_vw\n",
    "WHERE customer_id IS NULL;\n",
    "\n",
    "-- Chequeo 2: Verificar que no haya precios negativos (Validez)\n",
    "SELECT\n",
    "  COUNT(*) AS items_con_precio_negativo\n",
    "FROM order_items_bronze_vw\n",
    "WHERE price < 0;\n",
    "\n",
    "-- Chequeo 3: Verificar duplicados en clientes (Unicidad)\n",
    "SELECT\n",
    "  customer_id,\n",
    "  COUNT(*) as numero_de_apariciones\n",
    "FROM customers_bronze_vw\n",
    "GROUP BY customer_id\n",
    "HAVING COUNT(*) > 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6fadc85-0931-4dd6-894d-1400424b0e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En un pipeline real, si alguna de estas consultas devuelve un resultado mayor a cero, podríamos decidir detener la ejecución del notebook y enviar una alerta. Esto asegura que solo datos que cumplen con un estándar mínimo de calidad sean procesados y guardados en la capa Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0644ce1f-ae9b-42d0-8535-79a77a6a92f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Función de ayuda para ejecutar nuestras validaciones de calidad de datos\n",
    "def ejecutar_validacion_calidad(query, descripcion_validacion):\n",
    "  \"\"\"\n",
    "  Ejecuta una consulta SQL que se espera que devuelva 0.\n",
    "  Si devuelve un valor mayor a 0, lanza una excepción para detener el pipeline.\n",
    "  \n",
    "  :param query: La consulta SQL que cuenta las filas que violan la regla.\n",
    "  :param descripcion_validacion: Una descripción clara de lo que se está validando.\n",
    "  \"\"\"\n",
    "  print(f\"Ejecutando validación: '{descripcion_validacion}'...\")\n",
    "  \n",
    "  # Ejecutamos la consulta y obtenemos el resultado\n",
    "  df_resultado = spark.sql(query)\n",
    "  conteo_filas_malas = df_resultado.first()[0]\n",
    "\n",
    "  display(df_resultado)\n",
    "\n",
    "  display(conteo_filas_malas)\n",
    "  \n",
    "  # Verificamos si la validación falló\n",
    "  if conteo_filas_malas > 0:\n",
    "    # Si hay filas malas, lanzamos una excepción para detener todo.\n",
    "    # El mensaje de la excepción aparecerá en los logs del Job.\n",
    "    raise Exception(f\"VALIDACIÓN FALLIDA: {descripcion_validacion}. Se encontraron {conteo_filas_malas} filas que violan la regla.\")\n",
    "  else:\n",
    "    # Si todo está bien, imprimimos un mensaje de éxito.\n",
    "    print(f\"VALIDACIÓN EXITOSA: {descripcion_validacion}. No se encontraron problemas. ✔️\")\n",
    "\n",
    "# --- ¡Ahora usamos nuestra función para definir nuestras reglas! ---\n",
    "\n",
    "# REGLA 1: La llave primaria de un cliente NUNCA debe ser nula.\n",
    "query_pk_nula_cliente = \"SELECT COUNT(*) FROM customers_bronze_vw WHERE customer_id IS NULL\"\n",
    "ejecutar_validacion_calidad(query_pk_nula_cliente, \"La columna 'customer_id' en la tabla de clientes no debe contener nulos.\")\n",
    "\n",
    "# REGLA 2: El precio de un item NUNCA debe ser negativo.\n",
    "query_precio_negativo = \"SELECT COUNT(*) FROM order_items_bronze_vw WHERE price < 0\"\n",
    "ejecutar_validacion_calidad(query_precio_negativo, \"La columna 'price' en la tabla de items no debe contener valores negativos.\")\n",
    "\n",
    "# REGLA 3: El estado de un pedido debe ser uno de los valores conocidos.\n",
    "query_estado_invalido = \"\"\"\n",
    "  SELECT COUNT(*) \n",
    "  FROM orders_bronze_vw \n",
    "  WHERE order_status NOT IN ('delivered', 'shipped', 'canceled', 'invoiced', 'processing', 'approved', 'unavailable', 'created')\n",
    "\"\"\"\n",
    "ejecutar_validacion_calidad(query_estado_invalido, \"La columna 'order_status' contiene valores no permitidos.\")\n",
    "\n",
    "# REGLA 4: No debe haber pedidos sin al menos un item.\n",
    "query_pedidos_sin_items = \"\"\"\n",
    "  SELECT COUNT(o.order_id)\n",
    "  FROM orders_bronze_vw o\n",
    "  LEFT JOIN order_items_bronze_vw i ON o.order_id = i.order_id\n",
    "  WHERE i.order_id IS NULL\n",
    "\"\"\"\n",
    "ejecutar_validacion_calidad(query_pedidos_sin_items, \"Existen pedidos en la tabla 'orders' que no tienen ningún item asociado en 'order_items'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc61e40-39c1-4f45-a0d0-29104a55f234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Desglose de la Función `ejecutar_validacion_calidad`\n",
    "\n",
    "Aquí está el código comentado línea por línea para entender qué hace cada parte:\n",
    "\n",
    "```python\n",
    "# La función acepta dos argumentos:\n",
    "# 1. 'query': Una cadena de texto que contiene una consulta SQL.\n",
    "# 2. 'descripcion_validacion': Una explicación en lenguaje natural de la regla que estamos probando.\n",
    "def ejecutar_validacion_calidad(query, descripcion_validacion):\n",
    "  \"\"\"\n",
    "  Ejecuta una consulta SQL que se espera que devuelva 0.\n",
    "  Si devuelve un valor mayor a 0, lanza una excepción para detener el pipeline.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Imprimimos un mensaje para saber qué regla se está ejecutando en este momento.\n",
    "  # Esto es muy útil para depurar los logs de un Job.\n",
    "  print(f\"Ejecutando validación: '{descripcion_validacion}'...\")\n",
    "  \n",
    "  # Aquí usamos Spark para ejecutar la consulta SQL que le pasamos como argumento.\n",
    "  # El resultado de la consulta (que es una sola fila con una sola columna de conteo)\n",
    "  # se guarda en un DataFrame llamado 'df_resultado'.\n",
    "  df_resultado = spark.sql(query)\n",
    "  \n",
    "  # Un DataFrame es como una tabla. Para obtener el valor real del conteo, hacemos dos cosas:\n",
    "  # 1. .first(): Obtenemos la primera (y única) fila del DataFrame.\n",
    "  # 2. [0]: De esa fila, obtenemos el valor de la primera (y única) columna.\n",
    "  # Ahora, 'conteo_filas_malas' es un número (ej. 0, 5, 100).\n",
    "  conteo_filas_malas = df_resultado.first()[0]\n",
    "  \n",
    "  # --- AQUÍ ESTÁ EL CORAZÓN DE LA LÓGICA ---\n",
    "  # Comparamos el número que obtuvimos con 0.\n",
    "  if conteo_filas_malas > 0:\n",
    "    \n",
    "    # Si el conteo es MAYOR que cero, significa que encontramos datos que violan la regla.\n",
    "    # Esta es la parte crítica: 'raise Exception(...)'.\n",
    "    # 'raise' es la palabra clave de Python para generar un error intencionadamente.\n",
    "    # Al hacer esto, la ejecución del notebook se detiene por completo y se marca como \"Fallida\".\n",
    "    # El mensaje que escribimos aquí es el que aparecerá en el error del Job,\n",
    "    # diciéndonos exactamente qué falló y cuántas filas malas se encontraron.\n",
    "    raise Exception(f\"VALIDACIÓN FALLIDA: {descripcion_validacion}. Se encontraron {conteo_filas_malas} filas que violan la regla.\")\n",
    "  \n",
    "  else:\n",
    "    # Si el conteo es igual a cero, significa que no se encontraron filas malas.\n",
    "    # La regla se cumplió, así que imprimimos un mensaje de éxito y la función termina.\n",
    "    # El notebook continúa su ejecución hacia la siguiente celda.\n",
    "    print(f\"VALIDACIÓN EXITOSA: {descripcion_validacion}. No se encontraron problemas. ✔️\")\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## La Lógica en Acción: Un Ejemplo Práctico\n",
    "\n",
    "Imagina que llamamos a la función así:\n",
    "\n",
    "```python\n",
    "# REGLA: El precio de un item NUNCA debe ser negativo.\n",
    "query_precio_negativo = \"SELECT COUNT(*) FROM order_items_bronze_vw WHERE price < 0\"\n",
    "ejecutar_validacion_calidad(query_precio_negativo, \"El precio no debe ser negativo.\")\n",
    "```\n",
    "\n",
    "**Escenario 1: Los datos están limpios**\n",
    "\n",
    "1.  La consulta `SELECT COUNT(*)...` se ejecuta y devuelve `0`.\n",
    "2.  `conteo_filas_malas` se convierte en `0`.\n",
    "3.  La condición `if 0 > 0` es **falsa**.\n",
    "4.  Se ejecuta el bloque `else`, imprimiendo el mensaje de \"VALIDACIÓN EXITOSA\".\n",
    "5.  El pipeline continúa felizmente hacia la siguiente fase.\n",
    "\n",
    "**Escenario 2: Hay 3 items con precio negativo**\n",
    "\n",
    "1.  La consulta `SELECT COUNT(*)...` se ejecuta y devuelve `3`.\n",
    "2.  `conteo_filas_malas` se convierte en `3`.\n",
    "3.  La condición `if 3 > 0` es **verdadera**.\n",
    "4.  Se ejecuta el bloque `if`, y la línea `raise Exception(...)` detiene todo.\n",
    "5.  El Job de Databricks se marca como **\"Failed\"** y en la notificación de error recibirás el mensaje: `\"VALIDACIÓN FALLIDA: El precio no debe ser negativo. Se encontraron 3 filas que violan la regla.\"`\n",
    "\n",
    "En resumen, esta función es un patrón de diseño simple pero increíblemente poderoso que te permite construir **pipelines de datos defensivos**, es decir, pipelines que se protegen a sí mismos de datos de mala calidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fec16160-0eaa-4881-9721-0bc6eb09baf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## ¿Qué Acaba de Pasar?\n",
    "\n",
    "El mensaje de error es muy claro:\n",
    "`\"VALIDACIÓN FALLIDA: Existen pedidos en la tabla 'orders' que no tienen ningún item asociado en 'order_items'. Se encontraron 775 filas que violan la regla.\"`\n",
    "\n",
    "Esto significa que tu guardián de calidad de datos revisó los datos crudos de la capa Bronce y descubrió que hay **775 pedidos que no tienen ni un solo producto asociado**.\n",
    "\n",
    "### ¿Por Qué Esto es un Problema de Negocio?\n",
    "\n",
    "Un pedido sin productos es un dato anómalo. Podría significar varias cosas:\n",
    "\n",
    "  * **Registros Incompletos**: Un error en el sistema de origen que creó el pedido pero no registró los artículos.\n",
    "  * **Carritos Abandonados**: Podrían ser carritos que se iniciaron pero nunca se completaron.\n",
    "  * **Fraude o Pruebas**: Podrían ser registros de prueba o intentos de fraude.\n",
    "\n",
    "Si permitiéramos que estos 775 \"pedidos fantasma\" llegaran a nuestra capa Silver, cualquier análisis sobre el \"número total de pedidos\" estaría **inflado y sería incorrecto**. Un ejecutivo podría pensar que hubo 775 ventas más de las que realmente ocurrieron.\n",
    "\n",
    "-----\n",
    "\n",
    "## ¿Cómo se Soluciona en un Entorno Real?\n",
    "\n",
    "Ahora que el guardián ha hecho su trabajo, como arquitecto de datos, tienes que tomar una decisión. El siguiente paso es actuar.\n",
    "\n",
    "1.  **Investigar (¿Quiénes son?)**: El primer paso sería analizar esas 775 filas. ¿Son de un período de tiempo específico? ¿De un tipo de cliente? Esto nos daría pistas sobre la causa raíz.\n",
    "\n",
    "2.  **Decidir la Estrategia (¿Qué hacemos con ellos?)**:\n",
    "\n",
    "      * **Opción A (La más común): Filtrarlos.** Decidimos que estos registros no son válidos para el análisis y los excluimos de la capa Silver. Nos aseguramos de que solo los pedidos completos y válidos lleguen a nuestra fuente de la verdad.\n",
    "      * **Opción B (Avanzada): Ponerlos en Cuarentena.** Movemos estas 775 filas a una \"tabla de registros rechazados\" para que un equipo pueda analizarlos más tarde o corregirlos manualmente.\n",
    "      * **Opción C (Ideal): Corregir el Origen.** Contactamos al equipo que gestiona el sistema de e-commerce para informarles del problema y que lo solucionen en el sistema de origen.\n",
    "\n",
    "### La Solución para Nuestro Taller (Opción A)\n",
    "\n",
    "Para nuestro taller, la decisión es simple: **vamos a filtrar estos registros**.\n",
    "\n",
    "La buena noticia es que el código que escribimos para crear la `fact_pedidos` **ya hace esto implícitamente**. Observa la consulta:\n",
    "\n",
    "```sql\n",
    "FROM orders_bronze_vw o\n",
    "JOIN order_items_bronze_vw i ON o.order_id = i.order_id -- ¡Aquí está la clave!\n",
    "...\n",
    "```\n",
    "\n",
    "Al usar un `JOIN` (que por defecto es un `INNER JOIN`), ya estamos diciendo \"solo quiero los pedidos que tengan una correspondencia en la tabla de items\". Es decir, ya estábamos filtrando correctamente. La validación que creamos fue simplemente una forma de **hacer explícito y consciente** este problema de calidad de datos.\n",
    "\n",
    "**Para continuar, simplemente elimina o comenta la celda de validación que falló.** Hemos aprendido la lección: los datos crudos no son confiables, y nuestro `INNER JOIN` es la decisión de diseño correcta para asegurar la calidad en la capa Silver."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7836466378837949,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cuaderno 2: Data Quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
