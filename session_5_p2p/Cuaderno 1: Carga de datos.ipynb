{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65b60ef-0348-4e68-9bb7-2c44e555f534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 0: Creaci√≥n de la Arquitectura del Cat√°logo\n",
    "\n",
    "**Objetivo:** Construir el esqueleto de nuestro Lakehouse en Unity Catalog.\n",
    "\n",
    "Inspirados en las mejores pr√°cticas, no trabajaremos en esquemas sueltos. Crearemos una estructura autocontenida y profesional:\n",
    "1.  **Un Cat√°logo dedicado**: `sesion_5`. Ser√° el contenedor principal de todo nuestro proyecto.\n",
    "2.  **Tres Esquemas (Bases de Datos)**: `bronze`, `silver` y `gold`, uno para cada capa de nuestra arquitectura Medallion.\n",
    "3.  **Tres Vol√∫menes**: `raw_files`, `silver_tables` y `gold_tables`, para almacenar f√≠sicamente los datos de cada capa.\n",
    "\n",
    "Este paso se ejecuta una sola vez para configurar todo el entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ecc042-7811-42e4-a7e4-45544ce47354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## La Arquitectura Medallion: Un Enfoque L√≥gico para el Lakehouse\n",
    "\n",
    "La **Arquitectura Medallion** es un patr√≥n de dise√±o de datos que organiza l√≥gicamente los datos dentro de un Data Lakehouse en tres capas distintas: **Bronce (raw)**, **Plata (validated)** y **Oro (enriched)**. El objetivo principal es mejorar de manera incremental la calidad, estructura y fiabilidad de los datos a medida que fluyen a trav√©s de estas capas. \n",
    "\n",
    "Imagina el proceso como el de una refiner√≠a: el petr√≥leo crudo (Bronce) se refina para obtener productos m√°s √∫tiles y limpios como la gasolina (Plata), y finalmente se convierte en productos de alto valor y especializados (Oro).\n",
    "\n",
    "***\n",
    "### ¬øPor Qu√© Usar la Arquitectura Medallion? La Justificaci√≥n\n",
    "\n",
    "Adoptar este enfoque no es solo una cuesti√≥n de organizaci√≥n, sino una estrategia que resuelve problemas fundamentales en la gesti√≥n de datos a gran escala:\n",
    "\n",
    "* **Integridad y Confianza en los Datos**: Garantiza que los usuarios de negocio consuman datos de alta calidad (capa Oro), mientras que los procesos de ETL/ELT se construyen sobre una base validada y limpia (capa Plata). Esto evita el problema del \"pantano de datos\" (Data Swamp), donde los datos crudos y sin gobierno generan desconfianza.\n",
    "* **Linaje de Datos y Gobernanza Sencillos**: La estructura en capas hace que sea muy f√°cil rastrear el origen de cualquier dato (linaje) y aplicar controles de seguridad y gobernanza. Por ejemplo, se puede restringir el acceso a los datos crudos (Bronce) y solo permitir a la mayor√≠a de los usuarios acceder a las capas Plata y Oro.\n",
    "* **Eficiencia en el Procesamiento**: Al tener una capa Plata limpia y modelada, se evita tener que reprocesar y limpiar los datos crudos de la capa Bronce una y otra vez para cada nuevo caso de uso. La capa Plata act√∫a como una \"fuente √∫nica de la verdad\" optimizada para futuras transformaciones.\n",
    "* **Democratizaci√≥n del Acceso a los Datos**: Cada capa sirve a un prop√≥sito y a una audiencia diferente, permitiendo que tanto los ingenieros de datos, como los cient√≠ficos de datos y los analistas de negocio trabajen de manera eficiente sin interferir entre s√≠.\n",
    "\n",
    "***\n",
    "### Las Capas en Detalle\n",
    "\n",
    "#### ü•â **Capa Bronce (Raw Data)**\n",
    "\n",
    "Esta es la primera parada de los datos en el Lakehouse.\n",
    "\n",
    "* **Prop√≥sito**: Ingestar y almacenar datos de los sistemas de origen en su **formato nativo y sin procesar**. Es una copia fiel del origen.\n",
    "* **Caracter√≠sticas**:\n",
    "    * **Inmutable y de solo adici√≥n (append-only)**: Se guardan todos los datos hist√≥ricos, sin eliminar ni modificar nada. Si llega una actualizaci√≥n de una fila, se a√±ade como una nueva fila, preservando el historial completo.\n",
    "    * **Estructura del Origen**: La estructura de las tablas en esta capa debe reflejar la del sistema de origen para facilitar la conciliaci√≥n.\n",
    "    * **Base para la Reconstrucci√≥n**: Es la capa de respaldo. Si hay errores en las capas Plata u Oro, siempre se pueden reconstruir a partir de los datos crudos y confiables de la capa Bronce.\n",
    "* **Usuarios T√≠picos**: Principalmente ingenieros de datos que construyen los pipelines hacia la capa Plata.\n",
    "\n",
    "#### ü•à **Capa Plata (Cleansed & Conformed Data)**\n",
    "\n",
    "Esta capa es la \"fuente √∫nica de la verdad\" del Lakehouse.\n",
    "\n",
    "* **Prop√≥sito**: Proporcionar una visi√≥n limpia, validada y modelada de los datos empresariales clave. Aqu√≠ es donde los datos crudos se convierten en informaci√≥n de valor.\n",
    "* **Caracter√≠sticas**:\n",
    "    * **Limpieza de Datos**: Se manejan valores nulos, se corrigen tipos de datos y se eliminan duplicados.\n",
    "    * **Enriquecimiento y Conformaci√≥n**: Se unen datos de diferentes fuentes para crear una visi√≥n m√°s completa. Por ejemplo, se une un pedido con la informaci√≥n del cliente.\n",
    "    * **Modelado de Datos**: Es aqu√≠ donde com√∫nmente se aplica un **Esquema en Estrella**, creando tablas de hechos y dimensiones que son intuitivas y optimizadas para el an√°lisis.\n",
    "* **Usuarios T√≠picos**: Ingenieros de datos, cient√≠ficos de datos (para exploraci√≥n y feature engineering) y analistas de negocio avanzados.\n",
    "\n",
    "#### ü•á **Capa Oro (Curated Business-Level Data)**\n",
    "\n",
    "Esta es la capa final, optimizada para el consumo directo por parte del negocio.\n",
    "\n",
    "* **Prop√≥sito**: Entregar datos altamente refinados y agregados, listos para responder a preguntas de negocio espec√≠ficas.\n",
    "* **Caracter√≠sticas**:\n",
    "    * **Agregaciones de Negocio**: Las tablas en esta capa contienen m√©tricas pre-calculadas (ej. `ventas_totales_por_mes`, `clientes_activos_por_region`).\n",
    "    * **Optimizaci√≥n para el Rendimiento**: Los datos suelen estar desnormalizados y organizados por proyecto o √°rea de negocio para que las consultas de BI y los modelos de ML sean extremadamente r√°pidos.\n",
    "    * **Enfocado en el Caso de Uso**: Mientras que la capa Plata est√° modelada para toda la empresa, la capa Oro se centra en las necesidades de un departamento o proyecto espec√≠fico (Finanzas, Marketing, etc.).\n",
    "* **Usuarios T√≠picos**: Analistas de negocio, cient√≠ficos de datos (para entrenamiento de modelos) y cualquier usuario final que consuma dashboards o reportes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09243d5c-be91-447b-842e-a1599a0c6d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Creamos nuestro propio cat√°logo para el proyecto\n",
    "CREATE CATALOG IF NOT EXISTS sesion_5;\n",
    "\n",
    "-- 2. Creamos los esquemas para cada capa dentro de nuestro nuevo cat√°logo\n",
    "CREATE SCHEMA IF NOT EXISTS sesion_5.bronze COMMENT 'Esquema para datos crudos e inmutables';\n",
    "CREATE SCHEMA IF NOT EXISTS sesion_5.silver COMMENT 'Esquema para datos validados, limpiados y modelados';\n",
    "CREATE SCHEMA IF NOT EXISTS sesion_5.gold COMMENT 'Esquema para datos agregados y listos para el negocio';\n",
    "\n",
    "-- 3. Creamos los vol√∫menes para el almacenamiento f√≠sico de los datos\n",
    "CREATE VOLUME IF NOT EXISTS sesion_5.bronze.raw_files;\n",
    "CREATE VOLUME IF NOT EXISTS sesion_5.silver.tables;\n",
    "CREATE VOLUME IF NOT EXISTS sesion_5.gold.tables;\n",
    "\n",
    "SELECT \"Cat√°logo, esquemas y vol√∫menes creados exitosamente.\" as status;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268f85f9-fe49-4569-87ec-58924e149cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úã Acci√≥n Manual: Subir los Archivos a la Capa Bronze\n",
    "\n",
    "Ahora que la estructura est√° creada, debes subir los 9 archivos CSV del dataset de Olist a nuestro nuevo volumen.\n",
    "\n",
    "**Pasos:**\n",
    "1.  En el men√∫ de la izquierda, ve a **Catalog**.\n",
    "2.  Navega hasta tu nuevo cat√°logo: `sesion_5`.\n",
    "3.  Entra al esquema `bronze`.\n",
    "4.  Haz clic en el volumen `raw_files`.\n",
    "5.  Usa el bot√≥n **\"Upload to this volume\"** para subir tus archivos.\n",
    "\n",
    "**Una vez que los archivos est√©n subidos, puedes continuar ejecutando el resto del cuaderno.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d0db36-1051-4a90-bf20-2fac94c0a531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 1: Configuraci√≥n y Carga de Datos en DataFrames\n",
    "\n",
    "**Objetivo:** Definir las rutas a nuestra nueva arquitectura y cargar los datos del volumen `bronze` en DataFrames de Spark para su procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64bce3e9-1697-426a-81e1-7a0e73356ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Definir la configuraci√≥n usando nuestra nueva estructura de cat√°logo\n",
    "catalog_name = \"sesion_5\"\n",
    "bronze_schema = \"bronze\"\n",
    "silver_schema = \"silver\"\n",
    "\n",
    "# 2. Definir la ruta al volumen de la capa Bronze\n",
    "bronze_volume_path = f\"/Volumes/{catalog_name}/{bronze_schema}/raw_files\"\n",
    "\n",
    "# 3. Cargar todos los datasets desde el volumen Bronze en DataFrames\n",
    "print(f\"Leyendo archivos desde: {bronze_volume_path}...\")\n",
    "options = {\"header\": \"true\", \"inferSchema\": \"true\"}\n",
    "\n",
    "customers_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_customers_dataset.csv\")\n",
    "order_items_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_order_items_dataset.csv\")\n",
    "order_payments_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_order_payments_dataset.csv\")\n",
    "orders_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_orders_dataset.csv\")\n",
    "products_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_products_dataset.csv\")\n",
    "sellers_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/olist_sellers_dataset.csv\")\n",
    "category_translation_df = spark.read.options(**options).csv(f\"{bronze_volume_path}/product_category_name_translation.csv\")\n",
    "\n",
    "print(\"\\n¬°Carga completada! Los DataFrames de la capa Bronze est√°n en memoria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388bc067-b6bf-45f5-9a1c-e9bb00b5a001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 2: Exploraci√≥n de Datos (EDA)\n",
    "\n",
    "**Objetivo:** Entender la estructura, el contenido y la escala de nuestros datos crudos.\n",
    "\n",
    "Antes de transformar, debemos entender con qu√© estamos trabajando. Un buen EDA nos permite:\n",
    "1.  **Validar los Tipos de Datos**: ¬øSpark infiri√≥ correctamente las fechas y los n√∫meros?\n",
    "2.  **Identificar Nulos**: ¬øHay columnas con datos faltantes que debamos tratar?\n",
    "3.  **Tener una Noci√≥n del Volumen**: ¬øEstamos hablando de miles o millones de registros?\n",
    "\n",
    "Realizaremos una exploraci√≥n b√°sica sobre los DataFrames que acabamos de cargar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6fc3a5-00f8-4a22-86cc-b86ab82bd80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Revisar los esquemas para validar los tipos de datos\n",
    "print(\"--- Esquema de 'orders_df' (Pedidos) ---\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "# 2. Contar el n√∫mero de filas para entender la escala\n",
    "orders_count = orders_df.count()\n",
    "print(f\"\\nN√∫mero de registros en 'orders_df': {orders_count:,}\")\n",
    "\n",
    "# 3. Visualizar una muestra de los datos para entender el contenido\n",
    "print(\"\\n--- Muestra de datos de 'orders_df' ---\")\n",
    "orders_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b78d42b-4165-4017-9802-9e586db2833a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 3: Reflexi√≥n - El Plan del Arquitecto \n",
    "\n",
    "**Objetivo:** Dise√±ar nuestro modelo Silver antes de construirlo.\n",
    "\n",
    "Hemos confirmado que tenemos 9 DataFrames con datos relacionados pero desorganizados. Si un analista quisiera saber \"las ventas totales por ciudad\", tendr√≠a que unir `orders`, `order_items`, `order_payments` y `customers` cada vez que haga la consulta. Esto es ineficiente y propenso a errores.\n",
    "\n",
    "Aqu√≠ es donde aplicamos el **pensamiento arquitect√≥nico**. Nuestro plan es transformar estos datos crudos en un **Esquema en Estrella** robusto y optimizado para el an√°lisis.\n",
    "\n",
    "* **La Estrella Central (Hechos)**: El proceso de negocio fundamental es el **pedido**. Por lo tanto, crearemos una √∫nica tabla de hechos, `fact_pedidos`, que contendr√° las m√©tricas (`precio`, `valor_pago`) y las claves para conectarse con su contexto.\n",
    "* **Los Puntos de la Estrella (Dimensiones)**: El contexto que describe cada hecho son las entidades de negocio:\n",
    "    * `dim_clientes`: ¬ø**Qui√©n** compr√≥?\n",
    "    * `dim_productos`: ¬ø**Qu√©** se compr√≥?\n",
    "    * `dim_vendedores`: ¬ø**Qui√©n** lo vendi√≥?\n",
    "\n",
    "Este modelo es el objetivo de nuestra capa **Silver**. En la siguiente fase, lo haremos realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65a9410-7816-4d33-bda6-95a871b750ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 4: Creaci√≥n de Tablas Silver y Comparativa de M√©todos\n",
    "\n",
    "**Objetivo:** Crear nuestras tablas en la capa Silver y entender las dos formas de hacerlo: **PySpark vs. SQL**.\n",
    "\n",
    "### 4.1. PySpark vs. SQL: Dos Caminos para el Mismo Destino\n",
    "* **PySpark (Program√°tico)** üêç: Flexible y potente, ideal para l√≥gica compleja y pipelines automatizados. Manipulamos DataFrames con c√≥digo Python.\n",
    "* **Spark SQL (Declarativo)** üìä: Legible y universal. Perfecto para transformaciones de BI. Le *declaramos* el resultado que queremos a la base de datos.\n",
    "\n",
    "Crearemos la `dim_clientes` con ambos m√©todos para ver la diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f357e87a-9ae6-4080-8afa-7b1adbee712c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejemplo 1: Creando `dim_clientes` con PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee25aa7-2143-4390-bd41-497cd4c3d5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 1 (PySpark): Crear una tabla vac√≠a con el esquema correcto\n",
    "-- Usamos SQL para definir la estructura porque es muy claro y expl√≠cito.\n",
    "CREATE OR REPLACE TABLE sesion_5.silver.dim_clientes_pyspark (\n",
    "  customer_id STRING NOT NULL,\n",
    "  customer_unique_id STRING,\n",
    "  customer_zip_code_prefix STRING,\n",
    "  customer_city STRING,\n",
    "  customer_state STRING\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Dimensi√≥n de Clientes, creada con PySpark.';\n",
    "\n",
    "SELECT \"Tabla 'dim_clientes_pyspark' creada con el esquema correcto.\" as status;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc6da3a-3f40-4b9b-ad78-a5765ecb4eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Paso 2 (PySpark): Transformar los datos en un DataFrame\n",
    "dim_clientes_pyspark_df = customers_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_unique_id\"),\n",
    "    col(\"customer_zip_code_prefix\"),\n",
    "    col(\"customer_city\"),\n",
    "    col(\"customer_state\")\n",
    ").distinct()\n",
    "\n",
    "# Paso 3 (PySpark): Insertar los datos del DataFrame en la tabla que ya creamos.\n",
    "# Usamos .insertInto() para a√±adir los datos a la tabla existente.\n",
    "# El modo \"overwrite\" vaciar√° la tabla antes de insertar, asegurando que no haya datos viejos.\n",
    "dim_clientes_pyspark_df.write.mode(\"overwrite\").insertInto(\"sesion_5.silver.dim_clientes_pyspark\")\n",
    "\n",
    "print(\"Datos insertados correctamente en 'dim_clientes_pyspark'.\")\n",
    "display(spark.table(\"sesion_5.silver.dim_clientes_pyspark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48d238a-e0f1-40e5-91fb-e98e936c1f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejemplo 2: Creando `dim_clientes` con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d39e155-5ace-4c9c-bcd5-0f86236af138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Registrar el DataFrame como una vista temporal para poder usar SQL sobre √©l.\n",
    "customers_df.createOrReplaceTempView(\"customers_bronze_vw\")\n",
    "\n",
    "print(\"Vista temporal 'customers_bronze_vw' creada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0099fe6e-9cc9-4c48-84d9-3246a49b6d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Paso 2: Crear la tabla con el esquema expl√≠cito y luego insertar los datos.\n",
    "-- Este enfoque de dos pasos nos da control total sobre la estructura de la tabla.\n",
    "\n",
    "-- Primero, creamos la tabla con la columna de la llave primaria definida como NOT NULL.\n",
    "CREATE OR REPLACE TABLE sesion_5.silver.dim_clientes_sql (\n",
    "  customer_id STRING NOT NULL,\n",
    "  customer_unique_id STRING,\n",
    "  customer_zip_code_prefix STRING,\n",
    "  customer_city STRING,\n",
    "  customer_state STRING\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Dimensi√≥n de Clientes, creada con SQL y con esquema definido.';\n",
    "\n",
    "-- Segundo, insertamos los datos en la tabla ya creada.\n",
    "INSERT INTO sesion_5.silver.dim_clientes_sql\n",
    "SELECT DISTINCT\n",
    "  customer_id,\n",
    "  customer_unique_id,\n",
    "  customer_zip_code_prefix,\n",
    "  customer_city,\n",
    "  customer_state\n",
    "FROM customers_bronze_vw;\n",
    "\n",
    "-- Verificamos el resultado\n",
    "SELECT * FROM sesion_5.silver.dim_clientes_sql LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffb3102-b5e4-48f8-8a07-a023f675a33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora construiremos las tablas `dim_productos`, `dim_vendedores` y, finalmente, la tabla central `fact_pedidos` que une todo el modelo. Seguiremos el mismo patr√≥n de crear vistas temporales sobre los datos Bronze y luego usar SQL para transformar y crear las tablas Silver.\n",
    "\n",
    "### 5.1. Dimensi√≥n de Productos (`dim_productos`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cba8787-7bc1-4e93-9a12-85dab12999ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Registrar los DataFrames necesarios como vistas temporales\n",
    "products_df.createOrReplaceTempView(\"products_bronze_vw\")\n",
    "category_translation_df.createOrReplaceTempView(\"category_translation_bronze_vw\")\n",
    "\n",
    "print(\"Vistas temporales 'products_bronze_vw' y 'category_translation_bronze_vw' creadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1419429-898c-4c9e-8435-33cd58f6d605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Crear la tabla de dimensi√≥n de productos\n",
    "-- **CORRECCI√ìN**: A√±adimos la definici√≥n de la columna PK como NOT NULL\n",
    "CREATE OR REPLACE TABLE sesion_5.silver.dim_productos (\n",
    "  product_id STRING NOT NULL,\n",
    "  product_category STRING,\n",
    "  product_photos_qty INT,\n",
    "  product_weight_g DOUBLE,\n",
    "  product_length_cm DOUBLE,\n",
    "  product_height_cm DOUBLE,\n",
    "  product_width_cm DOUBLE\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Dimensi√≥n de Productos, enriquecida.';\n",
    "\n",
    "-- Insertamos los datos desde nuestra vista temporal\n",
    "INSERT INTO sesion_5.silver.dim_productos\n",
    "SELECT DISTINCT\n",
    "  p.product_id,\n",
    "  t.product_category_name_english AS product_category,\n",
    "  p.product_photos_qty,\n",
    "  p.product_weight_g,\n",
    "  p.product_length_cm,\n",
    "  p.product_height_cm,\n",
    "  p.product_width_cm\n",
    "FROM products_bronze_vw p\n",
    "LEFT JOIN category_translation_bronze_vw t ON p.product_category_name = t.product_category_name;\n",
    "\n",
    "-- Verificamos\n",
    "SELECT * FROM sesion_5.silver.dim_productos LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5beae4a-2329-4a65-9825-6363fb1c35c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2. Dimensi√≥n de Vendedores (`dim_vendedores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b41a481-1d29-4aca-ad5e-263d79f92c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Registrar el DataFrame de vendedores como una vista temporal\n",
    "sellers_df.createOrReplaceTempView(\"sellers_bronze_vw\")\n",
    "\n",
    "print(\"Vista temporal 'sellers_bronze_vw' creada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4ce1f0-4fa8-4862-bc66-ad4750b3b91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Crear la tabla de dimensi√≥n de vendedores\n",
    "-- **CORRECCI√ìN**: A√±adimos la definici√≥n de la columna PK como NOT NULL\n",
    "CREATE OR REPLACE TABLE sesion_5.silver.dim_vendedores (\n",
    "  seller_id STRING NOT NULL,\n",
    "  seller_zip_code_prefix STRING,\n",
    "  seller_city STRING,\n",
    "  seller_state STRING\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Dimensi√≥n de Vendedores, depurada y modelada.';\n",
    "\n",
    "-- Insertamos los datos\n",
    "INSERT INTO sesion_5.silver.dim_vendedores\n",
    "SELECT DISTINCT\n",
    "  seller_id,\n",
    "  seller_zip_code_prefix,\n",
    "  seller_city,\n",
    "  seller_state\n",
    "FROM sellers_bronze_vw;\n",
    "\n",
    "-- Verificamos\n",
    "SELECT * FROM sesion_5.silver.dim_vendedores LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "335ad731-20f1-41eb-92b7-e4c62b746277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.3. Tabla de Hechos de Pedidos (`fact_pedidos`)\n",
    "Esta es la tabla m√°s importante. La construiremos uniendo los DataFrames de pedidos, items de pedidos y pagos para consolidar todas las m√©tricas y claves for√°neas en un solo lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a65bee-2b72-4776-8f57-1f40d61f3fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Registrar todos los DataFrames relacionados con los pedidos como vistas temporales\n",
    "orders_df.createOrReplaceTempView(\"orders_bronze_vw\")\n",
    "order_items_df.createOrReplaceTempView(\"order_items_bronze_vw\")\n",
    "order_payments_df.createOrReplaceTempView(\"order_payments_bronze_vw\")\n",
    "\n",
    "print(\"Vistas temporales para pedidos, items y pagos creadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51dede37-c418-450c-8c7f-aca7c9376bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Crear la tabla de hechos\n",
    "-- **CORRECCI√ìN**: Definimos las columnas que ser√°n FK como NOT NULL\n",
    "CREATE OR REPLACE TABLE sesion_5.silver.fact_pedidos (\n",
    "  order_id STRING,\n",
    "  customer_id STRING NOT NULL,\n",
    "  product_id STRING NOT NULL,\n",
    "  seller_id STRING NOT NULL,\n",
    "  fecha_pedido DATE,\n",
    "  order_status STRING,\n",
    "  price DOUBLE,\n",
    "  costo_envio DOUBLE,\n",
    "  secuencia_pago INT,\n",
    "  tipo_pago STRING,\n",
    "  cuotas_pago INT,\n",
    "  valor_pago DOUBLE\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Tabla de hechos con las m√©tricas de los pedidos.';\n",
    "\n",
    "-- Insertamos los datos\n",
    "INSERT INTO sesion_5.silver.fact_pedidos\n",
    "SELECT\n",
    "  o.order_id,\n",
    "  o.customer_id,\n",
    "  i.product_id,\n",
    "  i.seller_id,\n",
    "  TO_DATE(o.order_purchase_timestamp) AS fecha_pedido,\n",
    "  o.order_status,\n",
    "  i.price,\n",
    "  i.freight_value AS costo_envio,\n",
    "  p.payment_sequential AS secuencia_pago,\n",
    "  p.payment_type AS tipo_pago,\n",
    "  p.payment_installments AS cuotas_pago,\n",
    "  p.payment_value AS valor_pago\n",
    "FROM orders_bronze_vw o\n",
    "JOIN order_items_bronze_vw i ON o.order_id = i.order_id\n",
    "JOIN order_payments_bronze_vw p ON o.order_id = p.order_id;\n",
    "\n",
    "-- Verificamos\n",
    "SELECT * FROM sesion_5.silver.fact_pedidos LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d86649b9-14f4-4467-b009-e0eb9e9456de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 6: Definici√≥n de Claves Primarias y For√°neas\n",
    "\n",
    "**Objetivo:** Establecer formalmente la integridad y las relaciones de nuestro modelo.\n",
    "\n",
    "Ahora que las tablas existen, vamos a a√±adirles \"restricciones\" (constraints).\n",
    "* **Llaves Primarias (PK)**: Garantizan que cada fila en una tabla de dimensi√≥n sea √∫nica.\n",
    "* **Llaves For√°neas (FK)**: Crean un v√≠nculo entre la tabla de hechos y las tablas de dimensi√≥n, asegurando que un pedido solo pueda referenciar a clientes y productos que realmente existen.\n",
    "\n",
    "En el Lakehouse, estas restricciones son informacionales (`NOT ENFORCED`), pero son vitales para que las herramientas de BI, los optimizadores de consultas y los propios desarrolladores entiendan c√≥mo se relacionan las tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2cc0bc-bfd5-4b8c-97b0-b471fecd261e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Definiendo las Llaves Primarias para cada tabla de dimensi√≥n\n",
    "-- (Usamos la tabla creada con SQL como principal para el resto del taller)\n",
    "ALTER TABLE sesion_5.silver.dim_clientes_sql ADD CONSTRAINT pk_dim_clientes PRIMARY KEY(customer_id) NOT ENFORCED;\n",
    "ALTER TABLE sesion_5.silver.dim_productos ADD CONSTRAINT pk_dim_productos PRIMARY KEY(product_id) NOT ENFORCED;\n",
    "ALTER TABLE sesion_5.silver.dim_vendedores ADD CONSTRAINT pk_dim_vendedores PRIMARY KEY(seller_id) NOT ENFORCED;\n",
    "\n",
    "-- Definiendo las Llaves For√°neas en la tabla de hechos\n",
    "ALTER TABLE sesion_5.silver.fact_pedidos ADD CONSTRAINT fk_pedidos_clientes FOREIGN KEY(customer_id) REFERENCES sesion_5.silver.dim_clientes_sql(customer_id) NOT ENFORCED;\n",
    "ALTER TABLE sesion_5.silver.fact_pedidos ADD CONSTRAINT fk_pedidos_productos FOREIGN KEY(product_id) REFERENCES sesion_5.silver.dim_productos(product_id) NOT ENFORCED;\n",
    "ALTER TABLE sesion_5.silver.fact_pedidos ADD CONSTRAINT fk_pedidos_vendedores FOREIGN KEY(seller_id) REFERENCES sesion_5.silver.dim_vendedores(seller_id) NOT ENFORCED;\n",
    "\n",
    "SELECT \"Llaves primarias y for√°neas definidas exitosamente.\" as status;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7060d784-bd00-48d4-80ac-6752640a2d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 7: Verificaci√≥n Final del Modelo Silver\n",
    "\n",
    "**Objetivo:** Comprobar que nuestro modelo en estrella funciona correctamente y que las relaciones est√°n bien definidas.\n",
    "\n",
    "Ahora que tenemos un modelo de datos √≠ntegro y bien estructurado, podemos responder a preguntas de negocio complejas con consultas simples y eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2848a4e1-94ca-46e5-8a3b-f1775d39634c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Pregunta de negocio: ¬øCu√°l es el valor total pagado por categor√≠a de producto en el estado de S√£o Paulo (SP)?\n",
    "\n",
    "SELECT\n",
    "  dp.product_category,\n",
    "  SUM(fp.valor_pago) AS valor_total_pagado\n",
    "FROM sesion_5.silver.fact_pedidos AS fp\n",
    "JOIN sesion_5.silver.dim_clientes_sql AS dc ON fp.customer_id = dc.customer_id\n",
    "JOIN sesion_5.silver.dim_productos AS dp ON fp.product_id = dp.product_id\n",
    "WHERE\n",
    "  dc.customer_state = 'SP'\n",
    "GROUP BY\n",
    "  dp.product_category\n",
    "ORDER BY\n",
    "  valor_total_pagado DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7580f0-1e84-49d4-89bb-bb3cebd64d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## La Capa Oro: La \"√öltima Milla\" de los Datos ü•á\n",
    "\n",
    "Si la capa Plata es la \"fuente √∫nica de la verdad\" para toda la empresa, la capa **Oro** es la **\"versi√≥n de la verdad optimizada para un prop√≥sito espec√≠fico\"**. Su objetivo no es la integridad o el modelado general, sino la **velocidad y la facilidad de uso** para un caso de negocio concreto.\n",
    "\n",
    "Pi√©nsalo como la diferencia entre una cocina de restaurante bien surtida (la capa Plata) y el plato final que se sirve en la mesa (la capa Oro). El plato final ya tiene todos los ingredientes combinados, cocinados y presentados de la forma exacta en que el comensal lo necesita.\n",
    "\n",
    "***\n",
    "\n",
    "### ## Diferencias Clave: Plata vs. Oro\n",
    "\n",
    "| Caracter√≠stica | ü•à Capa Plata (Fuente de la Verdad) | ü•á Capa Oro (Consumo) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Prop√≥sito** | Integraci√≥n y modelado de datos para toda la empresa. | Responder a preguntas de negocio espec√≠ficas. |\n",
    "| **Estructura** | Generalmente modelada (ej. Esquema en Estrella). | Altamente desnormalizada y agregada. |\n",
    "| **Audiencia** | Ingenieros de datos, cient√≠ficos de datos. | Analistas de negocio, ejecutivos (v√≠a dashboards). |\n",
    "| **Alcance** | Empresarial (toda la informaci√≥n de clientes). | Espec√≠fico del proyecto (ej. \"rentabilidad del cliente\"). |\n",
    "| **Optimizaci√≥n** | Para la integridad y la flexibilidad del an√°lisis. | Para la velocidad de las consultas (`queries`). |\n",
    "\n",
    "***\n",
    "\n",
    "### ## Ejemplos Pr√°cticos para Nuestro Dataset de Olist\n",
    "\n",
    "A partir de nuestra capa Silver, podr√≠amos crear varias tablas Oro, cada una dise√±ada para un prop√≥sito diferente.\n",
    "\n",
    "#### Ejemplo 1: El Dashboard de Ventas (BI)\n",
    "* **Nombre de la tabla**: `resumen_ventas_mensual`\n",
    "* **Pregunta de negocio**: \"¬øC√≥mo van nuestras ventas mes a mes en cada estado?\"\n",
    "* **Estructura**:\n",
    "  * `a√±o_mes` (ej. '2018-02')\n",
    "  * `estado_cliente`\n",
    "  * `total_pedidos` (COUNT)\n",
    "  * `valor_total_vendido` (SUM)\n",
    "  * `promedio_por_pedido` (AVG)\n",
    "  * `total_clientes_unicos` (COUNT DISTINCT)\n",
    "* **Beneficio**: Una herramienta como Power BI o Tableau puede conectarse a esta tabla y crear un dashboard incre√≠blemente r√°pido, ya que todos los c√°lculos pesados ya est√°n hechos. El analista solo arrastra y suelta los campos.\n",
    "\n",
    "#### Ejemplo 2: El Modelo de Segmentaci√≥n de Clientes (Machine Learning)\n",
    "* **Nombre de la tabla**: `features_clientes_rfm`\n",
    "* **Pregunta de negocio**: \"¬øCu√°les son nuestros mejores clientes basados en su comportamiento de compra?\"\n",
    "* **Estructura (RFM - Recencia, Frecuencia, Valor Monetario)**:\n",
    "  * `customer_unique_id`\n",
    "  * `dias_desde_ultima_compra` (Recencia)\n",
    "  * `numero_total_pedidos` (Frecuencia)\n",
    "  * `valor_total_gastado` (Valor Monetario)\n",
    "  * `categoria_favorita`\n",
    "* **Beneficio**: Un cient√≠fico de datos puede usar esta tabla directamente para entrenar un modelo de clustering (ej. K-Means) para segmentar a los clientes en grupos como \"Campeones\", \"En Riesgo\", \"Nuevos\", etc. No necesita hacer `JOINs` ni agregaciones.\n",
    "\n",
    "#### Ejemplo 3: El Reporte Financiero\n",
    "* **Nombre de la tabla**: `performance_vendedores`\n",
    "* **Pregunta de negocio**: \"¬øQu√© vendedores est√°n generando m√°s ingresos y cu√°l es su costo de env√≠o promedio?\"\n",
    "* **Estructura**:\n",
    "  * `seller_id`\n",
    "  * `seller_city`\n",
    "  * `seller_state`\n",
    "  * `total_ingresos_generados`\n",
    "  * `promedio_costo_envio`\n",
    "  * `numero_productos_vendidos`\n",
    "* **Beneficio**: El equipo de finanzas o de operaciones puede evaluar r√°pidamente el rendimiento de los vendedores sin necesidad de consultar m√∫ltiples tablas.\n",
    "\n",
    "***\n",
    "\n",
    "### ## ¬øPor Qu√© es Tan Importante la Capa Oro?\n",
    "\n",
    "1.  **Rendimiento Extremo**: Las consultas a tablas agregadas son √≥rdenes de magnitud m√°s r√°pidas que las consultas que deben procesar millones de filas en la capa Silver. Para un dashboard interactivo, esta es la diferencia entre una experiencia fluida y una frustrante.\n",
    "2.  **Reducci√≥n de Costos**: En sistemas de `data warehousing` en la nube que cobran por datos escaneados, consultar tablas Oro peque√±as y agregadas es mucho m√°s barato que escanear tablas de hechos masivas repetidamente.\n",
    "3.  **Consistencia del Negocio**: Si todos los reportes de ventas se basan en la tabla `resumen_ventas_mensual`, te aseguras de que todos en la empresa vean exactamente los mismos n√∫meros. Evita que cada analista calcule las \"ventas totales\" a su manera.\n",
    "4.  **Autoservicio Real**: Empodera a los usuarios de negocio para que puedan responder a sus propias preguntas sin depender constantemente del equipo de datos para construir consultas complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b942899f-f4e4-47a1-a8b6-a4aa1b3dd83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fase 8: Construcci√≥n de la Capa Oro (Gold)\n",
    "\n",
    "**Objetivo:** Crear tablas agregadas y espec√≠ficas para el negocio que potencien el BI y el reporting.\n",
    "\n",
    "La capa Oro es la √∫ltima milla de nuestro pipeline. No contiene datos nuevos, sino **agregaciones de la capa Silver** dise√±adas para ser extremadamente r√°pidas y f√°ciles de consultar.\n",
    "\n",
    "Crearemos una tabla llamada `ventas_por_estado_categoria` que pre-calcular√° las ventas totales. Un analista de negocio podr√≠a conectar una herramienta como Power BI directamente a esta tabla sin necesidad de escribir `JOINs` complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c4ae37-1ebf-44e1-8bbc-f9bbd120b9ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creamos nuestra primera tabla en la capa Gold\n",
    "CREATE OR REPLACE TABLE sesion_5.gold.ventas_por_estado_categoria\n",
    "USING DELTA\n",
    "COMMENT 'Tabla agregada con el total de ventas por estado del cliente y categor√≠a del producto.'\n",
    "AS\n",
    "SELECT\n",
    "  dc.customer_state,\n",
    "  dp.product_category,\n",
    "  COUNT(fp.order_id) AS total_pedidos,\n",
    "  SUM(fp.valor_pago) AS valor_total_pagado,\n",
    "  AVG(fp.valor_pago) AS promedio_pago_por_pedido\n",
    "FROM sesion_5.silver.fact_pedidos AS fp\n",
    "JOIN sesion_5.silver.dim_clientes_sql AS dc ON fp.customer_id = dc.customer_id\n",
    "JOIN sesion_5.silver.dim_productos AS dp ON fp.product_id = dp.product_id\n",
    "GROUP BY\n",
    "  dc.customer_state,\n",
    "  dp.product_category;\n",
    "\n",
    "-- ¬°Ahora consultar esta informaci√≥n es instant√°neo!\n",
    "SELECT *\n",
    "FROM sesion_5.gold.ventas_por_estado_categoria\n",
    "WHERE customer_state = 'SP'\n",
    "ORDER BY valor_total_pagado DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c784b742-f0e4-4ff9-8236-188cbe9c72ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ¬°Hemos Completado el Pipeline Medallion! üéâ\n",
    "\n",
    "**¬°Felicidades!** Has construido un pipeline de datos de extremo a extremo, desde la ingesta de archivos crudos (Bronze), pasando por la limpieza y modelado (Silver), hasta la creaci√≥n de una tabla agregada lista para el negocio (Gold).\n",
    "\n",
    "### Pr√≥ximos Pasos y Consideraciones Finales\n",
    "\n",
    "En un proyecto real, los siguientes pasos ser√≠an:\n",
    "* **Optimizaci√≥n y Mantenimiento**: Para asegurar que las consultas a la capa Silver sigan siendo r√°pidas a medida que los datos crecen, programar√≠amos tareas de mantenimiento como `OPTIMIZE` y `ZORDER`. Por ejemplo:\n",
    "    ```sql\n",
    "    OPTIMIZE sesion_5.silver.fact_pedidos ZORDER BY (fecha_pedido);\n",
    "    ```\n",
    "* **Orquestaci√≥n**: Automatizar√≠amos la ejecuci√≥n de este notebook para que se actualice diariamente usando una herramienta como Databricks Workflows o Airflow.\n",
    "* **Data Quality**: A√±adir√≠amos pruebas de calidad de datos (usando `EXPECTATIONS` en Delta Live Tables, por ejemplo) para asegurar que solo datos v√°lidos pasen de Bronze a Silver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ff942bc-f925-461f-b24b-5f5ffa16f4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "¬°Excelente! Ahora que tienes el pipeline completo, el siguiente paso l√≥gico es automatizarlo. La **orquestaci√≥n** es el proceso de programar y gestionar la ejecuci√≥n de tus flujos de trabajo de datos (tus notebooks) de manera autom√°tica, fiable y monitorizada.\n",
    "\n",
    "La herramienta nativa para esto en Databricks se llama **Databricks Workflows (Jobs)**. Es extremadamente potente y f√°cil de usar.\n",
    "\n",
    "Aqu√≠ tienes un paso a paso detallado para orquestar tu pipeline.\n",
    "\n",
    "---\n",
    "## Prerrequisito: Preparar tu Notebook para Producci√≥n\n",
    "\n",
    "Antes de automatizar, es una buena pr√°ctica \"limpiar\" tu notebook de desarrollo para crear una versi√≥n de producci√≥n. Un notebook de producci√≥n no deber√≠a tener celdas de exploraci√≥n o pruebas.\n",
    "\n",
    "**Acciones recomendadas:**\n",
    "1.  **Guarda una copia:** Guarda tu notebook actual con un nuevo nombre, por ejemplo, `pipeline_olist_produccion`.\n",
    "2.  **Elimina celdas innecesarias:** En la nueva copia, elimina las celdas que no son esenciales para la ejecuci√≥n del pipeline:\n",
    "    * Las celdas de exploraci√≥n (EDA) con `.show()` o `.printSchema()`.\n",
    "    * La celda de comparaci√≥n entre PySpark y SQL. Qu√©date solo con la versi√≥n que usar√°s en producci√≥n (la versi√≥n SQL en nuestro caso).\n",
    "    * La celda de `Fase 0` para crear el cat√°logo y los esquemas. En producci√≥n, la infraestructura ya deber√≠a existir.\n",
    "3.  **Aseg√∫rate de que se ejecute de principio a fin:** El notebook final debe contener √∫nicamente el c√≥digo necesario para ejecutar la transformaci√≥n de Bronze a Gold sin errores.\n",
    "\n",
    "---\n",
    "## Paso a Paso para la Orquestaci√≥n con Databricks Workflows\n",
    "\n",
    "### Paso 1: Ir a la Secci√≥n de Workflows\n",
    "\n",
    "En el men√∫ principal de la izquierda de tu workspace de Databricks, haz clic en el √≠cono de **Workflows**.\n",
    "\n",
    "\n",
    "---\n",
    "### Paso 2: Crear un Nuevo \"Job\"\n",
    "\n",
    "Un **Job** (Trabajo) es el contenedor de tu flujo de trabajo. Puede tener una o varias tareas.\n",
    "\n",
    "1.  Dentro de la p√°gina de Workflows, haz clic en el bot√≥n azul **Create Job**.\n",
    "2.  Dale un nombre descriptivo a tu Job en la parte superior, por ejemplo, `Procesamiento Diario Olist`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Paso 3: Configurar tu Primera Tarea (Task)\n",
    "\n",
    "Ahora, configurar√°s la tarea que ejecutar√° tu notebook.\n",
    "\n",
    "1.  **Nombre de la Tarea**: Dale un nombre a la tarea, como `bronze_a_gold`.\n",
    "2.  **Tipo (Type)**: Selecciona **Notebook**.\n",
    "3.  **Fuente (Source)**: Selecciona **Workspace** y busca tu notebook de producci√≥n (`pipeline_olist_produccion.ipynb`) usando el explorador de archivos.\n",
    "4.  **Cluster**: Aqu√≠ tienes la decisi√≥n m√°s importante.\n",
    "    * **Opci√≥n A (Recomendada para producci√≥n):** *New Job Cluster*. Esto crea un cl√∫ster temporal que se enciende solo para ejecutar el job y se apaga al terminar. Es la opci√≥n m√°s eficiente en costos. Puedes configurar un tama√±o de cl√∫ster peque√±o para empezar.\n",
    "    * **Opci√≥n B (Para pruebas):** Puedes seleccionar tu cl√∫ster interactivo existente (el que usaste para desarrollar).\n",
    "5.  Haz clic en el bot√≥n **Create task**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Paso 4: Programar la Ejecuci√≥n (Schedule)\n",
    "\n",
    "Aqu√≠ es donde le dices a Databricks *cu√°ndo* quieres que se ejecute tu pipeline.\n",
    "\n",
    "1.  En el panel de la derecha de la configuraci√≥n del Job, haz clic en **Add trigger**.\n",
    "2.  En \"Trigger type\", selecciona **Schedule**.\n",
    "3.  Configura la frecuencia. Por ejemplo, para una ejecuci√≥n diaria a las 3:00 AM:\n",
    "    * **Schedule**: `Daily`\n",
    "    * **At**: `03` : `00`\n",
    "4.  Puedes usar la sintaxis **Cron** para programaciones m√°s complejas si lo necesitas.\n",
    "5.  Haz clic en **Save**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Paso 5: (Opcional pero recomendado) Configurar Alertas\n",
    "\n",
    "Es vital saber si tu pipeline fall√≥ o se complet√≥ con √©xito.\n",
    "\n",
    "1.  En la configuraci√≥n del Job, ve a la secci√≥n de **Notifications**.\n",
    "2.  Haz clic en **Add notification**.\n",
    "3.  A√±ade tu correo electr√≥nico para los eventos **On success** (√©xito) y, m√°s importante, **On failure** (fallo).\n",
    "4.  Haz clic en **Confirm**.\n",
    "\n",
    "---\n",
    "### Paso 6: Ejecutar y Monitorear\n",
    "\n",
    "¬°Listo! Ya tienes un pipeline orquestado.\n",
    "\n",
    "* Puedes hacer clic en **Run now** en la esquina superior derecha para probarlo inmediatamente.\n",
    "* En la pesta√±a **Runs**, podr√°s ver el historial de todas las ejecuciones, ver si tuvieron √©xito o fallaron, y acceder a los logs para depurar cualquier problema.\n",
    "\n",
    "Siguiendo estos pasos, has pasado de un script interactivo a un pipeline de datos automatizado, robusto y listo para producci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a85b3e-fc63-44ac-99cc-59b82273cd20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Orquestaci√≥n del Pipeline con Databricks Workflows ‚öôÔ∏è\n",
    "\n",
    "**Objetivo:** Automatizar la ejecuci√≥n de nuestro pipeline para que se actualice de forma regular sin intervenci√≥n manual.\n",
    "\n",
    "Un pipeline solo es √∫til si los datos est√°n frescos. La orquestaci√≥n es el proceso de programar nuestro notebook para que se ejecute autom√°ticamente. La herramienta nativa para esto es **Databricks Workflows (Jobs)**.\n",
    "\n",
    "**Pasos para Orquestar este Notebook:**\n",
    "1.  **Preparar una Versi√≥n de Producci√≥n**: Guarda una copia de este notebook (ej. `pipeline_olist_produccion`) y elimina las celdas de exploraci√≥n (EDA) y las de comparaci√≥n (PySpark vs SQL). El notebook final debe contener solo el c√≥digo esencial para ir de Bronce a Oro.\n",
    "2.  **Ir a Workflows**: En el men√∫ de Databricks, ve a la secci√≥n \"Workflows\".\n",
    "3.  **Crear un Nuevo Job**: Dale un nombre (ej. `Procesamiento Diario Olist`).\n",
    "4.  **Configurar la Tarea**:\n",
    "    * **Tipo**: Notebook.\n",
    "    * **Fuente**: Selecciona tu notebook de producci√≥n.\n",
    "    * **Cluster**: Configura un *New Job Cluster*. Esto es m√°s eficiente en costos, ya que el cl√∫ster se crea solo para la ejecuci√≥n y se apaga al terminar.\n",
    "5.  **Programar la Ejecuci√≥n (Schedule)**:\n",
    "    * A√±ade un \"Trigger\" de tipo \"Schedule\".\n",
    "    * Config√∫ralo para que se ejecute diariamente a la hora que prefieras (ej. 3:00 AM).\n",
    "6.  **Configurar Alertas**: A√±ade tu correo electr√≥nico en la secci√≥n de \"Notifications\" para recibir una alerta si el job falla.\n",
    "\n",
    "Una vez configurado, Databricks se encargar√° de ejecutar tu pipeline y mantener tus datos actualizados autom√°ticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f871baf6-9628-477b-9256-e3075b5d9c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mantenimiento y Optimizaci√≥n a Largo Plazo üõ†Ô∏è\n",
    "\n",
    "**Objetivo:** Entender las tareas necesarias para mantener el rendimiento de nuestro Lakehouse.\n",
    "\n",
    "A medida que los datos crecen, las consultas pueden volverse m√°s lentas. Delta Lake viene con herramientas para combatir esto. En un entorno de producci√≥n, a√±adir√≠amos estos comandos al final de nuestro job orquestado:\n",
    "\n",
    "* **`OPTIMIZE`**: Compacta los archivos peque√±os en archivos m√°s grandes. Leer un archivo grande es mucho m√°s r√°pido para Spark que leer miles de archivos peque√±os.\n",
    "* **`ZORDER`**: Es una t√©cnica de co-localizaci√≥n de datos. Ordena los datos dentro de los archivos seg√∫n las columnas que m√°s usas para filtrar (como fechas o IDs), permitiendo a Spark \"saltarse\" los archivos que no necesita leer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61142867-289e-429c-8070-d1db4f86f821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ejemplo de comandos de mantenimiento que se ejecutar√≠an al final del pipeline\n",
    "-- Esto no necesita ejecutarse ahora, es para ilustrar el concepto.\n",
    "\n",
    "-- OPTIMIZE sesion_5.silver.fact_pedidos ZORDER BY (fecha_pedido);\n",
    "-- ANALYZE TABLE sesion_5.silver.fact_pedidos COMPUTE STATISTICS FOR ALL COLUMNS;\n",
    "\n",
    "SELECT \"El mantenimiento es clave para un rendimiento sostenido.\" as conclusion;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409d8c86-692d-4112-a53c-bfbd0966586a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusi√≥n de la Reconstrucci√≥n Punta a Punta üöÄ\n",
    "\n",
    "**¬°Lo hemos logrado!** Hemos construido un pipeline de datos completo, robusto y automatizable, siguiendo las mejores pr√°cticas de la industria.\n",
    "\n",
    "**Nuestro Viaje:**\n",
    "1.  **Configuramos una Arquitectura Profesional**: Creamos un cat√°logo dedicado con esquemas y vol√∫menes para cada capa (Bronce, Plata, Oro).\n",
    "2.  **Ingestamos Datos Crudos (Bronce)**: Cargamos los archivos CSV originales sin ninguna modificaci√≥n.\n",
    "3.  **Validamos la Calidad**: Implementamos \"guardianes\" que detienen el pipeline si los datos no cumplen con nuestras reglas de negocio, protegiendo la integridad de nuestro Lakehouse.\n",
    "4.  **Modelamos la Fuente de la Verdad (Plata)**: Transformamos los datos crudos en un Esquema en Estrella limpio, validado y con relaciones definidas, listo para an√°lisis flexibles.\n",
    "5.  **Entregamos Valor al Negocio (Oro)**: Creamos tablas pre-agregadas y optimizadas para que las consultas de BI sean instant√°neas.\n",
    "\n",
    "Ahora tienes un modelo de trabajo que puedes adaptar y escalar para cualquier proyecto de datos en el futuro."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7836466378837948,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cuaderno 1: Carga de datos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
